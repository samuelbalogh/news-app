[
  {
    "id": 653,
    "title": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving",
    "body": "Scaling Vision-Language-Action (VLA) models on large-scale data offers a\npromising path to achieving a more generalized driving intelligence. However,\nVLA models are limited by a ``supervision deficit'': the vast model capacity is\nsupervised by sparse, low-dimensional actions, leaving much of their\nrepresentational power underutilized. To remedy this, we propose\n\\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to\npredict future images. This task generates a dense, self-supervised signal that\ncompels the model to learn the underlying dynamics of the driving environment.\nWe showcase the paradigm's versatility by instantiating it for two dominant VLA\narchetypes: an autoregressive world model for VLAs that use discrete visual\ntokens, and a diffusion world model for those operating on continuous visual\nfeatures. Building on the rich representations learned from world modeling, we\nintroduce a lightweight action expert to address the inference latency for\nreal-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a\n680x larger in-house dataset demonstrate that DriveVLA-W0 significantly\noutperforms BEV and VLA baselines. Crucially, it amplifies the data scaling\nlaw, showing that performance gains accelerate as the training dataset size\nincreases.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.12796v1",
    "published_at": "2025-10-14T17:59:47",
    "created_at": "2025-10-15T09:00:32.868792",
    "key_finding": "drivevla-w0 improves driving intelligence by effectively utilizing the representation power of vision-language-action models through a novel world modeling approach, leading to superior performance on large datasets.",
    "methodology": "the framework employs self-supervised future image prediction via two vla archetypes—an autoregressive world model for discrete tokens and a diffusion world model for continuous features—coupled with a lightweight action expert for real-time inference.",
    "implications": "this approach not only enhances the model's ability to learn driving dynamics but also reveals that larger training datasets significantly amplify performance gains, highlighting the importance of scaling in developing autonomous driving technologies."
  },
  {
    "id": 654,
    "title": "CuMPerLay: Learning Cubical Multiparameter Persistence Vectorizations",
    "body": "We present CuMPerLay, a novel differentiable vectorization layer that enables\nthe integration of Cubical Multiparameter Persistence (CMP) into deep learning\npipelines. While CMP presents a natural and powerful way to topologically work\nwith images, its use is hindered by the complexity of multifiltration\nstructures as well as the vectorization of CMP. In face of these challenges, we\nintroduce a new algorithm for vectorizing MP homologies of cubical complexes.\nOur CuMPerLay decomposes the CMP into a combination of individual, learnable\nsingle-parameter persistence, where the bifiltration functions are jointly\nlearned. Thanks to the differentiability, its robust topological feature\nvectors can be seamlessly used within state-of-the-art architectures such as\nSwin Transformers. We establish theoretical guarantees for the stability of our\nvectorization under generalized Wasserstein metrics. Our experiments on\nbenchmark medical imaging and computer vision datasets show the benefit\nCuMPerLay on classification and segmentation performance, particularly in\nlimited-data scenarios. Overall, CuMPerLay offers a promising direction for\nintegrating global structural information into deep networks for structured\nimage analysis.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.12795v1",
    "published_at": "2025-10-14T17:59:01",
    "created_at": "2025-10-15T09:00:36.644209",
    "key_finding": "cumperlay introduces a differentiable vectorization layer for cubical multiparameter persistence (cmp) that improves classification and segmentation performance in deep learning models, especially under limited data conditions.",
    "methodology": "the approach involves a novel algorithm that decomposes cmp into learnable single-parameter persistence components, enabling effective integration into architectures like swin transformers while ensuring stability through theoretical guarantees.",
    "implications": "this work paves the way for better incorporation of topological features in deep learning frameworks, enhancing structured image analysis across various domains such as medical imaging and computer vision."
  },
  {
    "id": 655,
    "title": "UniFusion: Vision-Language Model as Unified Encoder in Image Generation",
    "body": "Although recent advances in visual generation have been remarkable, most\nexisting architectures still depend on distinct encoders for images and text.\nThis separation constrains diffusion models' ability to perform cross-modal\nreasoning and knowledge transfer. Prior attempts to bridge this gap often use\nthe last layer information from VLM, employ multiple visual encoders, or train\nlarge unified models jointly for text and image generation, which demands\nsubstantial computational resources and large-scale data, limiting its\naccessibility.We present UniFusion, a diffusion-based generative model\nconditioned on a frozen large vision-language model (VLM) that serves as a\nunified multimodal encoder. At the core of UniFusion is the Layerwise Attention\nPooling (LAP) mechanism that extracts both high level semantics and low level\ndetails from text and visual tokens of a frozen VLM to condition a diffusion\ngenerative model. We demonstrate that LAP outperforms other shallow fusion\narchitectures on text-image alignment for generation and faithful transfer of\nvisual information from VLM to the diffusion model which is key for editing. We\npropose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI),\nwhich conditions a diffusion transformer (DiT) only on the text tokens\ngenerated by the VLM during in-model prompt rewriting. VERIFI combines the\nalignment of the conditioning distribution with the VLM's reasoning\ncapabilities for increased capabilities and flexibility at inference. In\naddition, finetuning on editing task not only improves text-image alignment for\ngeneration, indicative of cross-modality knowledge transfer, but also exhibits\ntremendous generalization capabilities. Our model when trained on single image\nediting, zero-shot generalizes to multiple image references further motivating\nthe unified encoder design of UniFusion.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.12789v1",
    "published_at": "2025-10-14T17:57:56",
    "created_at": "2025-10-15T09:00:39.583852",
    "key_finding": "unifusion effectively utilizes a frozen vision-language model (vlm) as a unified multimodal encoder, significantly improving text-image alignment and visual information transfer in diffusion-based generation.",
    "methodology": "the model employs a layerwise attention pooling (lap) mechanism to extract semantic and detailed features from a vlm, along with the vlm-enabled rewriting injection with flexible inference (verifi) for enhanced conditioning during inference.",
    "implications": "unifusion's ability to generalize from single to multiple image references, coupled with improved editing capabilities, highlights the benefits of a unified encoder design in vision-language applications, making advanced cross-modal reasoning more accessible."
  },
  {
    "id": 656,
    "title": "ax-prover: a framework for theorem proving",
    "body": "We present Ax-Prover, a multi-agent system for automated theorem proving in\nLean that can solve problems across diverse scientific domains and operate\neither autonomously or collaboratively with human experts. To achieve this,\nAx-Prover approaches scientific problem solving through formal proof\ngeneration, a process that demands both creative reasoning and strict syntactic\nrigor. Ax-Prover meets this challenge by equipping Large Language Models\n(LLMs), which provide knowledge and reasoning, with Lean tools via the Model\nContext Protocol (MCP), which ensure formal correctness. To evaluate its\nperformance as an autonomous prover, we benchmark our approach against frontier\nLLMs and specialized prover models on two public math benchmarks and on two\nLean benchmarks we introduce in the fields of abstract algebra and quantum\ntheory. On public datasets, Ax-Prover is competitive with state-of-the-art\nprovers, while it largely outperform them on the new benchmarks. This shows\nthat, unlike specialized systems that struggle to generalize, our tool-based\nagentic theorem prover approach offers a generalizable methodology for formal\nverification across diverse scientific domains. Furthermore, we demonstrate\nAx-Prover's assistant capabilities in a practical use case, showing how it\nenabled an expert mathematician to formalize the proof of a complex\ncryptography theorem.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.12787v1",
    "published_at": "2025-10-14T17:57:04",
    "created_at": "2025-10-15T09:00:44.929108",
    "key_finding": "ax-prover is an advanced automated theorem prover that excels in diverse scientific domains, outperforming traditional models on newly introduced benchmarks.",
    "methodology": "the system integrates large language models with lean tools via the model context protocol to ensure formal proof generation and correctness, functioning either autonomously or in collaboration with human experts.",
    "implications": "this framework demonstrates a versatile approach to formal verification, offering significant potential for enhancing theorem proving in mathematics and quantum physics while supporting expert collaboration."
  },
  {
    "id": 657,
    "title": "MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars",
    "body": "Digital human avatars aim to simulate the dynamic appearance of humans in\nvirtual environments, enabling immersive experiences across gaming, film,\nvirtual reality, and more. However, the conventional process for creating and\nanimating photorealistic human avatars is expensive and time-consuming,\nrequiring large camera capture rigs and significant manual effort from\nprofessional 3D artists. With the advent of capable image and video generation\nmodels, recent methods enable automatic rendering of realistic animated avatars\nfrom a single casually captured reference image of a target subject. While\nthese techniques significantly lower barriers to avatar creation and offer\ncompelling realism, they lack constraints provided by multi-view information or\nan explicit 3D representation. So, image quality and realism degrade when\nrendered from viewpoints that deviate strongly from the reference image. Here,\nwe build a video model that generates animatable multi-view videos of digital\nhumans based on a single reference image and target expressions. Our model,\nMVP4D, is based on a state-of-the-art pre-trained video diffusion model and\ngenerates hundreds of frames simultaneously from viewpoints varying by up to\n360 degrees around a target subject. We show how to distill the outputs of this\nmodel into a 4D avatar that can be rendered in real-time. Our approach\nsignificantly improves the realism, temporal consistency, and 3D consistency of\ngenerated avatars compared to previous methods.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.12785v1",
    "published_at": "2025-10-14T17:56:14",
    "created_at": "2025-10-15T09:00:48.140655",
    "key_finding": "the mvp4d model enables the generation of highly realistic and animatable multi-view portrait videos of digital humans from a single reference image, enhancing the realism and consistency of animated avatars.",
    "methodology": "it utilizes a state-of-the-art pre-trained video diffusion model to generate hundreds of frames simultaneously, capturing variations in viewpoints up to 360 degrees, and distills the output into a real-time renderable 4d avatar.",
    "implications": "this advancement significantly reduces the traditional barriers of avatar creation, paving the way for immersive experiences in gaming, film, and virtual reality while ensuring high-quality visual output across diverse viewing angles."
  },
  {
    "id": 647,
    "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images",
    "body": "Recent advances in Large Language Models (LLMs) and Vision Language Models\n(VLMs) have shown significant progress in mathematical reasoning, yet they\nstill face a critical bottleneck with problems requiring visual assistance,\nsuch as drawing auxiliary lines or plotting functions to solve the problems.\nMost LLMs and VLMs are constrained to text-only reasoning chains, while\nmultimodal unified models that can generate interleaved text and images lack\nthe necessary precision and controllability for such tasks. To address this, we\npropose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking\nwith images\" in mathematics. Our approach leverages the VLM to generate text\nreasoning as well as executable plotting code, which is then rendered into\nimages as \"visual thought\", to solve mathematical problems. To achieve this, we\nfirst construct Math-VR, the first large-scale, bilingual dataset and benchmark\nfor Mathematics problems with Visual Reasoning, comprising 178K samples.\nSecond, to create high-quality training data, we develop a state-of-the-art\nimage-to-code converter specialized for parsing complex mathematical figures\ninto codes. Finally, using these training data, we train the CodePlot-CoT model\nfor solving mathematical problems. Experimental results show that our model\nachieves up to 21% increase over base model on our new benchmark, fully\nvalidating the efficacy of our proposed code-driven reasoning paradigm. Our\nwork opens a new direction for multimodal mathematical reasoning and provides\nthe community with the first large-scale dataset, comprehensive benchmark, and\nstrong approach for such problems. To facilitate future research, we make our\ndatasets, code, and pretrained models publicly available at\nhttps://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.11718v1",
    "published_at": "2025-10-13T17:59:55",
    "created_at": "2025-10-14T09:00:35.887996",
    "key_finding": "codeplot-cot significantly improves mathematical problem-solving by integrating visual reasoning with a code-driven approach, achieving up to a 21% performance increase over baseline models.",
    "methodology": "the research introduces a new chain-of-thought paradigm that generates executable plotting code from text reasoning, validated through a large-scale bilingual dataset of 178k visually-assisted math problems called math-vr.",
    "implications": "this work establishes a novel framework for multimodal mathematical reasoning and contributes essential resources, including datasets and pretrained models, to advance research in the field."
  },
  {
    "id": 648,
    "title": "Are Large Reasoning Models Interruptible?",
    "body": "Large Reasoning Models (LRMs) excel at complex reasoning but are\ntraditionally evaluated in static, \"frozen world\" settings: model responses are\nassumed to be instantaneous, and the context of a request is presumed to be\nimmutable over the duration of the response. While generally true for\nshort-term tasks, the \"frozen world\" assumption breaks down in modern reasoning\ntasks such as assistive programming, where models may take hours to think\nthrough problems and code may change dramatically from the time the model\nstarts thinking to the model's final output. In this work, we challenge the\nfrozen world assumption and evaluate LRM robustness under two realistic dynamic\nscenarios: interruptions, which test the quality of the model's partial outputs\non a limited budget, and dynamic context, which tests model adaptation to\nin-flight changes. Across mathematics and programming benchmarks that require\nlong-form reasoning, static evaluations consistently overestimate robustness:\neven state-of-the-art LRMs, which achieve high accuracy in static settings, can\nfail unpredictably when interrupted or exposed to changing context, with\nperformance dropping by up to 60% when updates are introduced late in the\nreasoning process. Our analysis further reveals several novel failure modes,\nincluding reasoning leakage, where models fold the reasoning into their final\nanswer when interrupted; panic, where under time pressure models abandon\nreasoning entirely and return incorrect answers; and self-doubt, where\nperformance degrades while incorporating updated information.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.11713v1",
    "published_at": "2025-10-13T17:59:35",
    "created_at": "2025-10-14T09:00:40.030326",
    "key_finding": "large reasoning models (lrms) exhibit significant performance drops, up to 60%, when faced with interruptions or dynamic context changes during long-form reasoning tasks.",
    "methodology": "the researchers evaluated lrm robustness through two dynamic scenarios—interruptions and dynamic context—using mathematics and programming benchmarks that require extended reasoning.",
    "implications": "this study challenges the traditional static evaluation of lrms, highlighting the need for new assessment methods that account for real-world applications where reasoning processes may be interrupted or context may change."
  },
  {
    "id": 652,
    "title": "Are Large Reasoning Models Interruptible?",
    "body": "Large Reasoning Models (LRMs) excel at complex reasoning but are\ntraditionally evaluated in static, \"frozen world\" settings: model responses are\nassumed to be instantaneous, and the context of a request is presumed to be\nimmutable over the duration of the response. While generally true for\nshort-term tasks, the \"frozen world\" assumption breaks down in modern reasoning\ntasks such as assistive programming, where models may take hours to think\nthrough problems and code may change dramatically from the time the model\nstarts thinking to the model's final output. In this work, we challenge the\nfrozen world assumption and evaluate LRM robustness under two realistic dynamic\nscenarios: interruptions, which test the quality of the model's partial outputs\non a limited budget, and dynamic context, which tests model adaptation to\nin-flight changes. Across mathematics and programming benchmarks that require\nlong-form reasoning, static evaluations consistently overestimate robustness:\neven state-of-the-art LRMs, which achieve high accuracy in static settings, can\nfail unpredictably when interrupted or exposed to changing context, with\nperformance dropping by up to 60% when updates are introduced late in the\nreasoning process. Our analysis further reveals several novel failure modes,\nincluding reasoning leakage, where models fold the reasoning into their final\nanswer when interrupted; panic, where under time pressure models abandon\nreasoning entirely and return incorrect answers; and self-doubt, where\nperformance degrades while incorporating updated information.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.11713v2",
    "published_at": "2025-10-13T17:59:35",
    "created_at": "2025-10-15T02:00:36.795162",
    "key_finding": "**** large reasoning models (lrms) exhibit significant performance degradation under interruptions and when adapting to dynamic contexts, with accuracy dropping by up to 60% compared to static evaluations.",
    "methodology": "**** the study evaluates lrm robustness in two dynamic scenarios: interruptions, assessing the quality of partial outputs, and dynamic context, analyzing adaptation to changes during reasoning tasks, using benchmarks in mathematics and programming.",
    "implications": "**** these findings highlight the limitations of static evaluations and underscore the need for improved lrm designs that can effectively handle real-world dynamics, particularly in complex reasoning tasks like assistive programming."
  },
  {
    "id": 649,
    "title": "Reinforced sequential Monte Carlo for amortised sampling",
    "body": "This paper proposes a synergy of amortised and particle-based methods for\nsampling from distributions defined by unnormalised density functions. We state\na connection between sequential Monte Carlo (SMC) and neural sequential\nsamplers trained by maximum-entropy reinforcement learning (MaxEnt RL), wherein\nlearnt sampling policies and value functions define proposal kernels and twist\nfunctions. Exploiting this connection, we introduce an off-policy RL training\nprocedure for the sampler that uses samples from SMC -- using the learnt\nsampler as a proposal -- as a behaviour policy that better explores the target\ndistribution. We describe techniques for stable joint training of proposals and\ntwist functions and an adaptive weight tempering scheme to reduce training\nsignal variance. Furthermore, building upon past attempts to use experience\nreplay to guide the training of neural samplers, we derive a way to combine\nhistorical samples with annealed importance sampling weights within a replay\nbuffer. On synthetic multi-modal targets (in both continuous and discrete\nspaces) and the Boltzmann distribution of alanine dipeptide conformations, we\ndemonstrate improvements in approximating the true distribution as well as\ntraining stability compared to both amortised and Monte Carlo methods.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.11711v1",
    "published_at": "2025-10-13T17:59:11",
    "created_at": "2025-10-14T09:00:43.289482",
    "key_finding": "the proposed method enhances sampling accuracy from unnormalised density functions by integrating amortised and particle-based techniques through a reinforcement learning framework.",
    "methodology": "the authors leverage a novel off-policy reinforcement learning approach to improve proposal efficiency in sequential monte carlo by utilizing samples from smc as a behaviour policy and implementing techniques for stable training of sampling policies and value functions.",
    "implications": "this research demonstrates significant stability and accuracy improvements in sampling from complex distributions, suggesting better applicability in various fields where accurate sampling from unnormalised densities is crucial."
  },
  {
    "id": 650,
    "title": "Adversarial Attacks Leverage Interference Between Features in Superposition",
    "body": "Fundamental questions remain about when and why adversarial examples arise in\nneural networks, with competing views characterising them either as artifacts\nof the irregularities in the decision landscape or as products of sensitivity\nto non-robust input features. In this paper, we instead argue that adversarial\nvulnerability can stem from efficient information encoding in neural networks.\nSpecifically, we show how superposition - where networks represent more\nfeatures than they have dimensions - creates arrangements of latent\nrepresentations that adversaries can exploit. We demonstrate that adversarial\nperturbations leverage interference between superposed features, making attack\npatterns predictable from feature arrangements. Our framework provides a\nmechanistic explanation for two known phenomena: adversarial attack\ntransferability between models with similar training regimes and class-specific\nvulnerability patterns. In synthetic settings with precisely controlled\nsuperposition, we establish that superposition suffices to create adversarial\nvulnerability. We then demonstrate that these findings persist in a ViT trained\non CIFAR-10. These findings reveal adversarial vulnerability can be a byproduct\nof networks' representational compression, rather than flaws in the learning\nprocess or non-robust inputs.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.11709v1",
    "published_at": "2025-10-13T17:59:02",
    "created_at": "2025-10-14T09:00:46.733840",
    "key_finding": "adversarial vulnerability in neural networks arises from the efficient encoding of information via superposition, which allows adversaries to exploit interference between superposed features.",
    "methodology": "the authors conducted experiments in synthetic settings to establish the relationship between superposition and adversarial vulnerability, and then validated their findings using a vision transformer model trained on the cifar-10 dataset.",
    "implications": "this research suggests that adversarial attacks are a consequence of the networks' representational compression rather than merely flaws in the learning process or sensitivity to non-robust inputs, highlighting the need for new approaches to improve network robustness."
  },
  {
    "id": 651,
    "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs",
    "body": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.11696v1",
    "published_at": "2025-10-13T17:55:09",
    "created_at": "2025-10-14T09:00:51.143914",
    "key_finding": "**** qerl improves the efficiency and effectiveness of reinforcement learning (rl) for large language models (llms) by integrating nvfp4 quantization and an adaptive quantization noise mechanism, achieving significant performance gains in rollout phases.",
    "methodology": "**** the framework utilizes low-rank adaptation (lora) alongside quantization to reduce memory overhead, while dynamically adjusting noise levels during training to enhance exploration and strategy discovery.",
    "implications": "**** qerl enables the training of a 32b llm on a single h100 80gb gpu, achieving accelerated training times, improved reward growth, and competitive accuracy compared to traditional methods, highlighting its potential for more resource-efficient rl applications in llms."
  },
  {
    "id": 642,
    "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
    "body": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.09608v1",
    "published_at": "2025-10-10T17:59:58",
    "created_at": "2025-10-13T20:00:41.795595",
    "key_finding": "streamingvlm demonstrates effective real-time understanding of infinite video streams, achieving a 66.18% win rate on the new inf-streams-eval benchmark while maintaining performance at 8 fps on a single gpu.",
    "methodology": "the model employs a unified framework with a compact kv cache, optimizing training with supervised fine-tuning on short video chunks to efficiently mimic the inference-time attention pattern.",
    "implications": "this approach not only addresses the latency and memory challenges of processing long videos but also boosts general visual question answering capabilities without requiring additional fine-tuning."
  },
  {
    "id": 643,
    "title": "Prompting Test-Time Scaling Is A Strong LLM Reasoning Data Augmentation",
    "body": "Large language models (LLMs) have demonstrated impressive reasoning\ncapabilities when provided with chain-of-thought exemplars, but curating large\nreasoning datasets remains laborious and resource-intensive. In this work, we\nintroduce Prompting Test-Time Scaling (P-TTS), a simple yet effective\ninference-time data augmentation strategy for enhancing LLM reasoning through\nfinetuning. Rather than collecting thousands or even millions of examples,\nP-TTS leverages a small pool of only 90 manually selected reasoning instances\nand systematically varies exemplar augmentation through principled instruction\nprompting intensities at test time to synthesize diverse reasoning trajectory\ncontexts. Then we finetune the various sizes of Qwen-2.5 models on P-TTS data.\nAcross a suite of mathematical reasoning AIME2024 & 25, MATH500, and\nGPQA-Diamond, our P-TTS-7B and 32B models outperform the prior competitive\nbaselines like S1 and S1.1 (1K-shot), achieving absolute accuracy gains of\n+26.66% and +30.00% on AIME'24 (7B), and +13.34% and +6.67% on AIME'25 (7B);\nP-TTS-32B yields gains of +23.33% and +16.63% on AIME'24, and +26.63% and\n+3.33% on AIME'25 (vs. S1 and S1.1, respectively), with comparable or better\nperformance on MATH500 and GPQA-Diamond. We further show that P-TTS enhances\nzero-shot generalization accuracy on out-of-domain reasoning benchmarks of\nGaokao, Kaoyan, OlympiadBench, AMC23, GradeSchoolMath, and Minerva. Our\nanalysis suggests that test-time scaling effectively explores the latent space\nof reasoning patterns, amplifying LLM problem-solving with minimal annotation\noverhead, and further unlocking the reasoning potential and capabilities of\nLLMs. Prompting Test-Time Scaling offers a practical, low-cost way to elicit\nLLM reasoning in resource-constrained or rapidly evolving domains.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.09599v1",
    "published_at": "2025-10-10T17:57:04",
    "created_at": "2025-10-13T20:00:44.634384",
    "key_finding": "prompting test-time scaling (p-tts) significantly enhances the reasoning accuracy of large language models (llms) through a novel data augmentation strategy.",
    "methodology": "the approach utilizes a small set of 90 manual reasoning instances, systematically varying exemplar augmentation at test time, and fine-tunes various sizes of qwen-2.5 models on this data.",
    "implications": "p-tts offers a cost-effective and efficient means to improve llm reasoning capabilities, making it applicable in resource-constrained environments and for rapid adaptation to new reasoning tasks."
  },
  {
    "id": 644,
    "title": "BaNEL: Exploration Posteriors for Generative Modeling Using Only Negative Rewards",
    "body": "Today's generative models thrive with large amounts of supervised data and\ninformative reward functions characterizing the quality of the generation. They\nwork under the assumptions that the supervised data provides knowledge to\npre-train the model, and the reward function provides dense information about\nhow to further improve the generation quality and correctness. However, in the\nhardest instances of important problems, two problems arise: (1) the base\ngenerative model attains a near-zero reward signal, and (2) calls to the reward\noracle are expensive. This setting poses a fundamentally different learning\nchallenge than standard reward-based post-training. To address this, we propose\nBaNEL (Bayesian Negative Evidence Learning), an algorithm that post-trains the\nmodel using failed attempts only, while minimizing the number of reward\nevaluations (NREs). Our method is based on the idea that the problem of\nlearning regularities underlying failures can be cast as another, in-loop\ngenerative modeling problem. We then leverage this model to assess whether new\ndata resembles previously seen failures and steer the generation away from\nthem. We show that BaNEL can improve model performance without observing a\nsingle successful sample on several sparse-reward tasks, outperforming existing\nnovelty-bonus approaches by up to several orders of magnitude in success rate,\nwhile using fewer reward evaluations.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.09596v1",
    "published_at": "2025-10-10T17:55:03",
    "created_at": "2025-10-13T20:00:48.707694",
    "key_finding": "**** banel significantly improves generative model performance in sparse-reward settings by utilizing only negative reward information and failed attempts, achieving higher success rates compared to existing methods.",
    "methodology": "**** the algorithm employs a bayesian approach to learn from failures and assess new data against previously encountered failures, allowing the model to avoid generating undesirable outcomes while reducing the number of reward evaluations required.",
    "implications": "**** this work presents a novel framework for generative modeling that can enhance performance in environments with limited feedback, making it particularly useful for problems where successful samples are rare or costly to obtain."
  },
  {
    "id": 645,
    "title": "LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?",
    "body": "Competitive programming problems increasingly serve as valuable benchmarks to\nevaluate the coding capabilities of large language models (LLMs) due to their\ncomplexity and ease of verification. Yet, current coding benchmarks face\nlimitations such as lack of exceptionally challenging problems, insufficient\ntest case coverage, reliance on online platform APIs that limit accessibility.\nTo address these issues, we introduce LiveOIBench, a comprehensive benchmark\nfeaturing 403 expert-curated Olympiad-level competitive programming problems,\neach with an average of 60 expert-designed test cases. The problems are sourced\ndirectly from 72 official Informatics Olympiads in different regions conducted\nbetween 2023 and 2025. LiveOIBench distinguishes itself through four key\nfeatures: (1) meticulously curated high-quality tasks with detailed subtask\nrubrics and extensive private test cases; (2) direct integration of elite\ncontestant performance data to enable informative comparison against\ntop-performing humans; (3) planned continuous, contamination-free updates from\nnewly released Olympiad problems; and (4) a self-contained evaluation system\nfacilitating offline and easy-to-reproduce assessments. Benchmarking 32 popular\ngeneral-purpose and reasoning LLMs, we find that GPT-5 achieves a notable\n81.76th percentile, a strong result that nonetheless falls short of top human\ncontestant performance, who usually place above 90th. In contrast, among\nopen-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile,\nunderscoring significant capability disparities from frontier closed models.\nDetailed analyses indicate that robust reasoning models prioritize precise\nproblem analysis over excessive exploration, suggesting future models should\nemphasize structured analysis and minimize unnecessary exploration. All data,\ncode, and leaderboard results will be made publicly available on our website.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.09595v1",
    "published_at": "2025-10-10T17:54:24",
    "created_at": "2025-10-13T20:00:51.356153",
    "key_finding": "liveoibench benchmark reveals that while gpt-5 performs impressively at the 81.76th percentile on competitive programming problems, it still lags behind top human contestants who consistently score above the 90th percentile.",
    "methodology": "the benchmark consists of 403 curated olympiad-level problems with extensive test cases, enabling performance comparisons between elite human contestants and 32 different llms, including both general-purpose and reasoning models.",
    "implications": "the results highlight the need for future llms to focus on structured problem analysis and limit exploratory behavior to enhance their coding capabilities in competitive programming contexts."
  },
  {
    "id": 646,
    "title": "MODE: Learning compositional representations of complex systems with Mixtures Of Dynamical Experts",
    "body": "Dynamical systems in the life sciences are often composed of complex mixtures\nof overlapping behavioral regimes. Cellular subpopulations may shift from\ncycling to equilibrium dynamics or branch towards different developmental\nfates. The transitions between these regimes can appear noisy and irregular,\nposing a serious challenge to traditional, flow-based modeling techniques which\nassume locally smooth dynamics. To address this challenge, we propose MODE\n(Mixture Of Dynamical Experts), a graphical modeling framework whose neural\ngating mechanism decomposes complex dynamics into sparse, interpretable\ncomponents, enabling both the unsupervised discovery of behavioral regimes and\naccurate long-term forecasting across regime transitions. Crucially, because\nagents in our framework can jump to different governing laws, MODE is\nespecially tailored to the aforementioned noisy transitions. We evaluate our\nmethod on a battery of synthetic and real datasets from computational biology.\nFirst, we systematically benchmark MODE on an unsupervised classification task\nusing synthetic dynamical snapshot data, including in noisy, few-sample\nsettings. Next, we show how MODE succeeds on challenging forecasting tasks\nwhich simulate key cycling and branching processes in cell biology. Finally, we\ndeploy our method on human, single-cell RNA sequencing data and show that it\ncan not only distinguish proliferation from differentiation dynamics but also\npredict when cells will commit to their ultimate fate, a key outstanding\nchallenge in computational biology.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.09594v1",
    "published_at": "2025-10-10T17:52:31",
    "created_at": "2025-10-13T20:00:54.306690",
    "key_finding": "the mode framework effectively captures complex dynamics in life sciences by decomposing them into interpretable behavioral regimes, enabling accurate forecasting during regime transitions.",
    "methodology": "mode utilizes a graphical modeling approach with a neural gating mechanism to facilitate unsupervised discovery of behaviors in noisy dynamical systems and to benchmark performance on synthetic and real biological datasets.",
    "implications": "this research not only enhances understanding of cellular behaviors like proliferation and differentiation but also offers a robust tool for predicting developmental fates in single-cell rna sequencing, addressing significant challenges in computational biology."
  },
  {
    "id": 637,
    "title": "BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation",
    "body": "Scaling data and models has played a pivotal role in the remarkable progress\nof computer vision and language. Inspired by these domains, recent efforts in\nrobotics have similarly focused on scaling both data and model size to develop\nmore generalizable and robust policies. However, unlike vision and language,\nrobotics lacks access to internet-scale demonstrations across diverse robotic\ntasks and environments. As a result, the scale of existing datasets typically\nsuffers from the need for manual data collection and curation. To address this\nproblem, here we propose BLAZER, a framework that learns manipulation policies\nfrom automatically generated training data. We build on the zero-shot\ncapabilities of LLM planners and automatically generate demonstrations for\ndiverse manipulation tasks in simulation. Successful examples are then used to\nfinetune an LLM and to improve its planning capabilities without human\nsupervision. Notably, while BLAZER training requires access to the simulator's\nstate, we demonstrate direct transfer of acquired skills to sensor-based\nmanipulation. Through extensive experiments, we show BLAZER to significantly\nimprove zero-shot manipulation in both simulated and real environments.\nMoreover, BLAZER improves on tasks outside of its training pool and enables\ndownscaling of LLM models. Our code and data will be made publicly available on\nthe project page.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.08572v1",
    "published_at": "2025-10-09T17:59:58",
    "created_at": "2025-10-10T09:00:28.975636",
    "key_finding": "blazer significantly enhances zero-shot manipulation capabilities for llm-based agents through the automatic generation of training data, leading to improved performance in both simulated and real environments.",
    "methodology": "the framework leverages large language model (llm) planners to generate diverse manipulation task demonstrations in a simulator, which are then used to finetune the llm without human supervision.",
    "implications": "this approach not only allows for direct transfer of skills to sensor-based manipulation but also facilitates training on tasks outside its original dataset, thus promoting the development of more generalizable robotic policies and enabling the downscaling of llm models."
  },
  {
    "id": 638,
    "title": "Reconstructing the local density field with combined convolutional and point cloud architecture",
    "body": "We construct a neural network to perform regression on the local dark-matter\ndensity field given line-of-sight peculiar velocities of dark-matter halos,\nbiased tracers of the dark matter field. Our architecture combines a\nconvolutional U-Net with a point-cloud DeepSets. This combination enables\nefficient use of small-scale information and improves reconstruction quality\nrelative to a U-Net-only approach. Specifically, our hybrid network recovers\nboth clustering amplitudes and phases better than the U-Net on small scales.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.08573v1",
    "published_at": "2025-10-09T17:59:58",
    "created_at": "2025-10-10T09:00:31.721493",
    "key_finding": "the developed hybrid neural network outperforms traditional u-net architectures in reconstructing the local dark-matter density field, particularly excelling on small scales.",
    "methodology": "the architecture integrates a convolutional u-net with a point-cloud deepsets approach to leverage both small-scale features and spatial arrangements effectively.",
    "implications": "this method enhances the precision of cosmological simulations and analyses, offering improved insights into the structure of the universe and the behavior of dark matter."
  },
  {
    "id": 639,
    "title": "Who Said Neural Networks Aren't Linear?",
    "body": "Neural networks are famously nonlinear. However, linearity is defined\nrelative to a pair of vector spaces, $f$$:$$X$$\\to$$Y$. Is it possible to\nidentify a pair of non-standard vector spaces for which a conventionally\nnonlinear function is, in fact, linear? This paper introduces a method that\nmakes such vector spaces explicit by construction. We find that if we sandwich\na linear operator $A$ between two invertible neural networks, $f(x)=g_y^{-1}(A\ng_x(x))$, then the corresponding vector spaces $X$ and $Y$ are induced by newly\ndefined addition and scaling actions derived from $g_x$ and $g_y$. We term this\nkind of architecture a Linearizer. This framework makes the entire arsenal of\nlinear algebra, including SVD, pseudo-inverse, orthogonal projection and more,\napplicable to nonlinear mappings. Furthermore, we show that the composition of\ntwo Linearizers that share a neural network is also a Linearizer. We leverage\nthis property and demonstrate that training diffusion models using our\narchitecture makes the hundreds of sampling steps collapse into a single step.\nWe further utilize our framework to enforce idempotency (i.e. $f(f(x))=f(x)$)\non networks leading to a globally projective generative model and to\ndemonstrate modular style transfer.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.08570v1",
    "published_at": "2025-10-09T17:59:57",
    "created_at": "2025-10-10T09:00:35.709450",
    "key_finding": "the paper demonstrates that conventional nonlinear neural networks can be transformed into linear operators by defining non-standard vector spaces, enabling the use of linear algebra techniques on non-linear mappings.",
    "methodology": "the authors introduce a framework called linearizer, where a linear operator is sandwiched between two invertible neural networks, creating new vector spaces through derived actions from the networks.",
    "implications": "this approach facilitates advanced applications in neural networks, such as simplifying diffusion model training and enforcing idempotency in networks for globally projective generative models and modular style transfer."
  },
  {
    "id": 640,
    "title": "NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos",
    "body": "Enabling robots to execute novel manipulation tasks zero-shot is a central\ngoal in robotics. Most existing methods assume in-distribution tasks or rely on\nfine-tuning with embodiment-matched data, limiting transfer across platforms.\nWe present NovaFlow, an autonomous manipulation framework that converts a task\ndescription into an actionable plan for a target robot without any\ndemonstrations. Given a task description, NovaFlow synthesizes a video using a\nvideo generation model and distills it into 3D actionable object flow using\noff-the-shelf perception modules. From the object flow, it computes relative\nposes for rigid objects and realizes them as robot actions via grasp proposals\nand trajectory optimization. For deformable objects, this flow serves as a\ntracking objective for model-based planning with a particle-based dynamics\nmodel. By decoupling task understanding from low-level control, NovaFlow\nnaturally transfers across embodiments. We validate on rigid, articulated, and\ndeformable object manipulation tasks using a table-top Franka arm and a Spot\nquadrupedal mobile robot, and achieve effective zero-shot execution without\ndemonstrations or embodiment-specific training. Project website:\nhttps://novaflow.lhy.xyz/.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.08568v1",
    "published_at": "2025-10-09T17:59:55",
    "created_at": "2025-10-10T09:00:39.014103",
    "key_finding": "novaflow enables robots to perform zero-shot manipulation tasks by converting task descriptions into actionable plans without requiring demonstrations or embodiment-specific training.",
    "methodology": "the framework synthesizes task-specific videos, distills them into 3d actionable object flows, and computes object poses for robot actions through grasp proposals and model-based planning.",
    "implications": "this approach enhances the versatility of robotic manipulation across different platforms and object types, facilitating broader applications in robotics without the need for fine-tuning on specific data."
  },
  {
    "id": 641,
    "title": "ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive Evaluation",
    "body": "Benchmarks are central to measuring the capabilities of large language models\nand guiding model development, yet widespread data leakage from pretraining\ncorpora undermines their validity. Models can match memorized content rather\nthan demonstrate true generalization, which inflates scores, distorts\ncross-model comparisons, and misrepresents progress. We introduce ArenaBencher,\na model-agnostic framework for automatic benchmark evolution that updates test\ncases while preserving comparability. Given an existing benchmark and a diverse\npool of models to be evaluated, ArenaBencher infers the core ability of each\ntest case, generates candidate question-answer pairs that preserve the original\nobjective, verifies correctness and intent with an LLM as a judge, and\naggregates feedback from multiple models to select candidates that expose\nshared weaknesses. The process runs iteratively with in-context demonstrations\nthat steer generation toward more challenging and diagnostic cases. We apply\nArenaBencher to math problem solving, commonsense reasoning, and safety domains\nand show that it produces verified, diverse, and fair updates that uncover new\nfailure modes, increase difficulty while preserving test objective alignment,\nand improve model separability. The framework provides a scalable path to\ncontinuously evolve benchmarks in step with the rapid progress of foundation\nmodels.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.08569v1",
    "published_at": "2025-10-09T17:59:55",
    "created_at": "2025-10-10T09:00:42.478646",
    "key_finding": "arenabencher effectively evolves benchmarks for large language models, ensuring they remain valid by exposing shared weaknesses and improving model separability.",
    "methodology": "the framework utilizes a multi-model competitive evaluation approach that generates and verifies new test cases iteratively while aligning with the original benchmark objectives.",
    "implications": "this scalable solution enables continuous benchmark evolution, allowing for more accurate assessments of model capabilities in rapidly advancing ai fields."
  },
  {
    "id": 632,
    "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
    "body": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.07318v1",
    "published_at": "2025-10-08T17:59:55",
    "created_at": "2025-10-09T09:00:38.811959",
    "key_finding": "the artificial hippocampus network (ahn) framework significantly enhances long-sequence modeling by effectively balancing short-term and long-term memory, achieving superior performance while reducing computational costs.",
    "methodology": "ahns utilize a sliding window of the transformer's kv cache for short-term memory and a recurrent module for compressing out-of-window data into a compact long-term memory, tested across various rnn-like architectures.",
    "implications": "this approach not only outperforms traditional sliding window baselines but also competes with full-attention models, suggesting a promising direction for efficient long-context modeling in ai applications."
  },
  {
    "id": 633,
    "title": "Vibe Checker: Aligning Code Evaluation with Human Preference",
    "body": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage\nLLMs to generate and iteratively refine code through natural language\ninteractions until it passes their vibe check. Vibe check is tied to real-world\nhuman preference and goes beyond functionality: the solution should feel right,\nread cleanly, preserve intent, and remain correct. However, current code\nevaluation remains anchored to pass@k and captures only functional correctness,\noverlooking the non-functional instructions that users routinely apply. In this\npaper, we hypothesize that instruction following is the missing piece\nunderlying vibe check that represents human preference in coding besides\nfunctional correctness. To quantify models' code instruction following\ncapabilities with measurable signals, we present VeriCode, a taxonomy of 30\nverifiable code instructions together with corresponding deterministic\nverifiers. We use the taxonomy to augment established evaluation suites,\nresulting in Vibe Checker, a testbed to assess both code instruction following\nand functional correctness. Upon evaluating 31 leading LLMs, we show that even\nthe strongest models struggle to comply with multiple instructions and exhibit\nclear functional regression. Most importantly, a composite score of functional\ncorrectness and instruction following correlates the best with human\npreference, with the latter emerging as the primary differentiator on\nreal-world programming tasks. Our work identifies core factors of the vibe\ncheck, providing a concrete path for benchmarking and developing models that\nbetter align with user preferences in coding.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.07315v1",
    "published_at": "2025-10-08T17:59:19",
    "created_at": "2025-10-09T09:00:42.481774",
    "key_finding": "**** the study reveals that current evaluations of code generated by large language models (llms) do not adequately capture human preferences, as they focus solely on functional correctness, neglecting essential non-functional aspects.",
    "methodology": "",
    "implications": "**methodology:** the authors introduce vericode, a taxonomy of 30 verifiable code instructions and deterministic verifiers, to create vibe checker—a testing framework that evaluates both code instruction following and functional correctness across 31 leading llms."
  },
  {
    "id": 634,
    "title": "GyroSwin: 5D Surrogates for Gyrokinetic Plasma Turbulence Simulations",
    "body": "Nuclear fusion plays a pivotal role in the quest for reliable and sustainable\nenergy production. A major roadblock to viable fusion power is understanding\nplasma turbulence, which significantly impairs plasma confinement, and is vital\nfor next-generation reactor design. Plasma turbulence is governed by the\nnonlinear gyrokinetic equation, which evolves a 5D distribution function over\ntime. Due to its high computational cost, reduced-order models are often\nemployed in practice to approximate turbulent transport of energy. However,\nthey omit nonlinear effects unique to the full 5D dynamics. To tackle this, we\nintroduce GyroSwin, the first scalable 5D neural surrogate that can model 5D\nnonlinear gyrokinetic simulations, thereby capturing the physical phenomena\nneglected by reduced models, while providing accurate estimates of turbulent\nheat transport.GyroSwin (i) extends hierarchical Vision Transformers to 5D,\n(ii) introduces cross-attention and integration modules for latent\n3D$\\leftrightarrow$5D interactions between electrostatic potential fields and\nthe distribution function, and (iii) performs channelwise mode separation\ninspired by nonlinear physics. We demonstrate that GyroSwin outperforms widely\nused reduced numerics on heat flux prediction, captures the turbulent energy\ncascade, and reduces the cost of fully resolved nonlinear gyrokinetics by three\norders of magnitude while remaining physically verifiable. GyroSwin shows\npromising scaling laws, tested up to one billion parameters, paving the way for\nscalable neural surrogates for gyrokinetic simulations of plasma turbulence.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.07314v1",
    "published_at": "2025-10-08T17:59:10",
    "created_at": "2025-10-09T09:00:45.240698",
    "key_finding": "gyroswin is a novel scalable 5d neural surrogate that accurately models nonlinear gyrokinetic plasma turbulence simulations and improves heat flux predictions compared to traditional reduced-order models.",
    "methodology": "the approach leverages extended hierarchical vision transformers with innovative cross-attention and integration modules to handle 5d nonlinear dynamics, alongside channelwise mode separation reflective of nonlinear physics.",
    "implications": "by significantly reducing computational costs and enhancing predictive accuracy, gyroswin facilitates more reliable modeling of plasma turbulence, crucial for the advancement of nuclear fusion energy technologies."
  },
  {
    "id": 635,
    "title": "h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement Learning",
    "body": "Large language models excel at short-horizon reasoning tasks, but performance\ndrops as reasoning horizon lengths increase. Existing approaches to combat this\nrely on inference-time scaffolding or costly step-level supervision, neither of\nwhich scales easily. In this work, we introduce a scalable method to bootstrap\nlong-horizon reasoning capabilities using only existing, abundant short-horizon\ndata. Our approach synthetically composes simple problems into complex,\nmulti-step dependency chains of arbitrary length. We train models on this data\nusing outcome-only rewards under a curriculum that automatically increases in\ncomplexity, allowing RL training to be scaled much further without saturating.\nEmpirically, our method generalizes remarkably well: curriculum training on\ncomposed 6th-grade level math problems (GSM8K) boosts accuracy on longer,\ncompetition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x.\nImportantly, our long-horizon improvements are significantly higher than\nbaselines even at high pass@k, showing that models can learn new reasoning\npaths under RL. Theoretically, we show that curriculum RL with outcome rewards\nachieves an exponential improvement in sample complexity over full-horizon\ntraining, providing training signal comparable to dense supervision. h1\ntherefore introduces an efficient path towards scaling RL for long-horizon\nproblems using only existing data.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.07312v1",
    "published_at": "2025-10-08T17:58:41",
    "created_at": "2025-10-09T09:00:49.341516",
    "key_finding": "**** this research demonstrates a method to enhance long-horizon reasoning in language models using synthetic problem composition and outcome-based reinforcement learning, achieving up to 2.06x accuracy improvements on complex benchmarks.",
    "methodology": "",
    "implications": "**methodology:** the approach involves creating multi-step dependency chains from simpler problems, training models with a curriculum that progressively increases complexity, and leveraging existing short-horizon data without the need for costly supervision."
  },
  {
    "id": 636,
    "title": "MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline",
    "body": "While Language Models (LMs) have made significant progress in automating\nmachine learning engineering (MLE), the acquisition of high-quality MLE\ntraining data is significantly constrained. Current MLE benchmarks suffer from\nlow scalability and limited applicability because they rely on static, manually\ncurated tasks, demanding extensive time and manual effort to produce. We\nintroduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw\ndatasets into competition-style MLE challenges through an efficient\ngenerate-verify-execute paradigm for scaling MLE tasks with verifiable quality,\nreal-world usability, and rich diversity. The proposed multi-agent pipeline in\nMLE-Smith drives structured task design and standardized refactoring, coupled\nwith a hybrid verification mechanism that enforces strict structural rules and\nhigh-level semantic soundness. It further validates empirical solvability and\nreal-world fidelity through interactive execution. We apply MLE-Smith to 224 of\nreal-world datasets and generate 606 tasks spanning multiple categories,\nobjectives, and modalities, demonstrating that MLE-Smith can work effectively\nacross a wide range of real-world datasets. Evaluation on the generated tasks\nshows that the performance of eight mainstream and cutting-edge LLMs on\nMLE-Smith tasks is strongly correlated with their performance on carefully\nhuman-designed tasks, highlighting the effectiveness of the MLE-Smith to\nscaling up MLE tasks, while maintaining task quality.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.07307v1",
    "published_at": "2025-10-08T17:57:19",
    "created_at": "2025-10-09T09:00:52.865925",
    "key_finding": "**** mle-smith is a fully automated multi-agent pipeline that successfully scales mle tasks by transforming raw datasets into high-quality, diverse competition-style challenges.",
    "methodology": "",
    "implications": "**methodology:** the pipeline employs a generate-verify-execute paradigm that incorporates structured task design, standardized refactoring, and a hybrid verification mechanism to ensure empirical solvability and semantic soundness across tasks derived from 224 real-world datasets."
  },
  {
    "id": 627,
    "title": "EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark",
    "body": "Most existing benchmarks for egocentric vision understanding focus primarily\non daytime scenarios, overlooking the low-light conditions that are inevitable\nin real-world applications. To investigate this gap, we present EgoNight, the\nfirst comprehensive benchmark for nighttime egocentric vision, with visual\nquestion answering (VQA) as the core task. A key feature of EgoNight is the\nintroduction of day-night aligned videos, which enhance night annotation\nquality using the daytime data and reveal clear performance gaps between\nlighting conditions. To achieve this, we collect both synthetic videos rendered\nby Blender and real-world recordings, ensuring that scenes and actions are\nvisually and temporally aligned. Leveraging these paired videos, we construct\nEgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and\nrefinement through extensive human verification. Each QA pair is double-checked\nby annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs\nacross 90 videos, spanning 12 diverse QA types, with more than 300 hours of\nhuman work. Evaluations of state-of-the-art multimodal large language models\n(MLLMs) reveal substantial performance drops when transferring from day to\nnight, underscoring the challenges of reasoning under low-light conditions.\nBeyond VQA, EgoNight also introduces two auxiliary tasks, day-night\ncorrespondence retrieval and egocentric depth estimation at night, that further\nexplore the boundaries of existing models. We believe EgoNight-VQA provides a\nstrong foundation for advancing application-driven egocentric vision research\nand for developing models that generalize across illumination domains. All the\ndata and code will be made available upon acceptance.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.06218v1",
    "published_at": "2025-10-07T17:59:47",
    "created_at": "2025-10-08T09:00:35.493568",
    "key_finding": "**** egonight is the first comprehensive benchmark for nighttime egocentric vision, revealing significant performance gaps in visual question answering (vqa) when transitioning from day to night.",
    "methodology": "**** the benchmark is built using day-night aligned videos, comprising synthetic and real-world recordings, and includes a novel auto-labeling engine refined by human verification, resulting in 3658 qa pairs across 90 videos.",
    "implications": "**** egonight not only highlights the challenges in low-light reasoning for multimodal large language models but also lays a foundation for advancing egocentric vision applications and improving generalization across various lighting conditions."
  },
  {
    "id": 628,
    "title": "TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning",
    "body": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor enhancing the reasoning capabilities of large reasoning models (LRMs),\nparticularly in the context of test-time scaling (TTS). However, their\npotential for supervising LRMs on tabular reasoning domains remains\nunderexplored. Through detailed empirical analyses, we identify that existing\nPRMs, though widely adopted for supervising text-only reasoning steps, struggle\nwith table-specific operations such as sub-table retrieval and schema\ninteraction, leading to critical performance bottlenecks. To address this\nlimitation, we propose TaTToo, a novel table-grounded PRM framework that (i)\nreasons explicitly over tabular reasoning steps and (ii) integrates tool-based\nverification to provide precise reward supervision. Concretely, we first design\na scalable data curation pipeline that constructs over 60k high-quality\nstep-level annotations by integrating table verification rationales with\ntool-based executions. Building on the collected data, we train TaTToo with a\ndual-stage paradigm: cold-start supervised fine-tuning to capture tool-use\nreasoning patterns, followed by reinforcement learning with tool-grounded\nreward shaping to align our model with table-based verification. We provide a\ncomprehensive evaluation of the policy improvement induced by our newly\ndesigned PRM. Across 5 challenging tabular reasoning benchmarks covering\nnumerical reasoning, fact-checking, and data analysis, TaTToo improves\ndownstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines\nsuch as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong\ngeneralizability across diverse TTS strategies.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.06217v1",
    "published_at": "2025-10-07T17:59:41",
    "created_at": "2025-10-08T09:00:39.042670",
    "key_finding": "tattoo significantly enhances tabular reasoning performance of large reasoning models, achieving a 30.9% improvement over existing baselines.",
    "methodology": "the framework uses a dual-stage training paradigm that incorporates a large dataset of over 60k annotated reasoning steps and employs tool-based verification for reward shaping.",
    "implications": "this research demonstrates the effectiveness of integrating process reward models with tool-grounded strategies, providing a promising direction for improved performance in tabular reasoning tasks across various applications."
  },
  {
    "id": 629,
    "title": "Stratified GRPO: Handling Structural Heterogeneity in Reinforcement Learning of LLM Search Agents",
    "body": "Large language model (LLM) agents increasingly rely on external tools such as\nsearch engines to solve complex, multi-step problems, and reinforcement\nlearning (RL) has become a key paradigm for training them. However, the\ntrajectories of search agents are structurally heterogeneous, where variations\nin the number, placement, and outcomes of search calls lead to fundamentally\ndifferent answer directions and reward distributions. Standard policy gradient\nmethods, which use a single global baseline, suffer from what we identify and\nformalize as cross-stratum bias-an \"apples-to-oranges\" comparison of\nheterogeneous trajectories. This cross-stratum bias distorts credit assignment\nand hinders exploration of complex, multi-step search strategies. To address\nthis, we propose Stratified GRPO, whose central component, Stratified Advantage\nNormalization (SAN), partitions trajectories into homogeneous strata based on\ntheir structural properties and computes advantages locally within each\nstratum. This ensures that trajectories are evaluated only against their true\npeers. Our analysis proves that SAN eliminates cross-stratum bias, yields\nconditionally unbiased unit-variance estimates inside each stratum, and retains\nthe global unbiasedness and unit-variance properties enjoyed by standard\nnormalization, resulting in a more pure and scale-stable learning signal. To\nimprove practical stability under finite-sample regimes, we further linearly\nblend SAN with the global estimator. Extensive experiments on diverse\nsingle-hop and multi-hop question-answering benchmarks demonstrate that\nStratified GRPO consistently and substantially outperforms GRPO by up to 11.3\npoints, achieving higher training rewards, greater training stability, and more\neffective search policies. These results establish stratification as a\nprincipled remedy for structural heterogeneity in RL for LLM search agents.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.06214v1",
    "published_at": "2025-10-07T17:59:13",
    "created_at": "2025-10-08T09:00:42.693805",
    "key_finding": "**** stratified grpo, utilizing stratified advantage normalization (san), significantly reduces cross-stratum bias in rl training of llm search agents, leading to enhanced performance.",
    "methodology": "",
    "implications": "**methodology:** the approach partitions heterogeneous search trajectories into homogeneous strata for local advantage computation, ensuring effective credit assignment and blending with a global estimator to maintain practical stability."
  },
  {
    "id": 630,
    "title": "Training Dynamics Impact Post-Training Quantization Robustness",
    "body": "While post-training quantization is widely adopted for efficient deployment\nof large language models, the mechanisms underlying quantization robustness\nremain unclear. We conduct a comprehensive analysis of quantization degradation\nacross open-source language model training trajectories up to 32B parameters\nand 15T training tokens to accurately assess the relationship between training\ndynamics and quantization performance. Our key finding is that quantization\nerrors in large-scale training runs are driven by a complex interplay between\nlearning rate and other training hyperparameters. Specifically, once learning\nrates decay, validation loss and quantization error diverge, largely\nindependent of training data scale. To investigate interventions on the\ntraining dynamics and identify specific configurations that can modulate\nquantization robustness favorably, we train our own models in controlled\nexperiments up to 100B tokens. Our results challenge the assumption that\nincreasing dataset scale inherently compromises quantization effectiveness,\ndemonstrating instead that strategic training hyperparameter interventions can\nimprove quantization quality at scale.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.06213v1",
    "published_at": "2025-10-07T17:59:07",
    "created_at": "2025-10-08T09:00:46.182151",
    "key_finding": "quantization errors in large-scale model training are influenced by the interplay between learning rate and training hyperparameters, leading to divergence between validation loss and quantization error post-learning rate decay.",
    "methodology": "the study involves a comprehensive analysis of quantization degradation across various open-source language models and controlled experiments with models trained up to 100b tokens.",
    "implications": "the findings challenge the notion that merely increasing dataset scale undermines quantization effectiveness, suggesting that tailored adjustments in training hyperparameters can enhance quantization robustness even at larger scales."
  },
  {
    "id": 631,
    "title": "Modulation Discovery with Differentiable Digital Signal Processing",
    "body": "Modulations are a critical part of sound design and music production,\nenabling the creation of complex and evolving audio. Modern synthesizers\nprovide envelopes, low frequency oscillators (LFOs), and more parameter\nautomation tools that allow users to modulate the output with ease. However,\ndetermining the modulation signals used to create a sound is difficult, and\nexisting sound-matching / parameter estimation systems are often\nuninterpretable black boxes or predict high-dimensional framewise parameter\nvalues without considering the shape, structure, and routing of the underlying\nmodulation curves. We propose a neural sound-matching approach that leverages\nmodulation extraction, constrained control signal parameterizations, and\ndifferentiable digital signal processing (DDSP) to discover the modulations\npresent in a sound. We demonstrate the effectiveness of our approach on highly\nmodulated synthetic and real audio samples, its applicability to different DDSP\nsynth architectures, and investigate the trade-off it incurs between\ninterpretability and sound-matching accuracy. We make our code and audio\nsamples available and provide the trained DDSP synths in a VST plugin.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.06204v1",
    "published_at": "2025-10-07T17:56:24",
    "created_at": "2025-10-08T09:00:48.709620",
    "key_finding": "the proposed neural sound-matching approach effectively extracts modulation signals from both synthetic and real audio, enhancing interpretability without sacrificing accuracy.",
    "methodology": "the study employs modulation extraction, constrained control signal parameterizations, and differentiable digital signal processing (ddsp) to analyze and identify the underlying modulation curves in sounds.",
    "implications": "this research presents a significant advancement in sound design and music production, offering tools for better understanding and recreating complex sound modulation, with resources made accessible via code, audio samples, and a vst plugin."
  },
  {
    "id": 622,
    "title": "TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration",
    "body": "Graph Neural Networks (GNNs) have shown remarkable success across various\nscientific fields, yet their adoption in critical decision-making is often\nhindered by a lack of interpretability. Recently, intrinsically interpretable\nGNNs have been studied to provide insights into model predictions by\nidentifying rationale substructures in graphs. However, existing methods face\nchallenges when the underlying rationale subgraphs are complex and varied. In\nthis work, we propose TopInG: Topologically Interpretable Graph Learning, a\nnovel topological framework that leverages persistent homology to identify\npersistent rationale subgraphs. TopInG employs a rationale filtration learning\napproach to model an autoregressive generation process of rationale subgraphs,\nand introduces a self-adjusted topological constraint, termed topological\ndiscrepancy, to enforce a persistent topological distinction between rationale\nsubgraphs and irrelevant counterparts. We provide theoretical guarantees that\nour loss function is uniquely optimized by the ground truth under specific\nconditions. Extensive experiments demonstrate TopInG's effectiveness in\ntackling key challenges, such as handling variform rationale subgraphs,\nbalancing predictive performance with interpretability, and mitigating spurious\ncorrelations. Results show that our approach improves upon state-of-the-art\nmethods on both predictive accuracy and interpretation quality.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.05102v1",
    "published_at": "2025-10-06T17:59:44",
    "created_at": "2025-10-07T09:00:37.100873",
    "key_finding": "toping enhances the interpretability of graph neural networks by effectively identifying persistent rationale subgraphs using topological methods.",
    "methodology": "the approach utilizes persistent homology and a rationale filtration learning technique, coupled with a self-adjusted topological constraint, to create a distinction between relevant and irrelevant subgraphs during an autoregressive generation process.",
    "implications": "this framework improves predictive accuracy and interpretability in graph learning settings, addressing challenges related to complex rationale structures and spurious correlations, thereby facilitating more reliable decision-making in critical applications."
  },
  {
    "id": 623,
    "title": "Paper2Video: Automatic Video Generation from Scientific Papers",
    "body": "Academic presentation videos have become an essential medium for research\ncommunication, yet producing them remains highly labor-intensive, often\nrequiring hours of slide design, recording, and editing for a short 2 to 10\nminutes video. Unlike natural video, presentation video generation involves\ndistinctive challenges: inputs from research papers, dense multi-modal\ninformation (text, figures, tables), and the need to coordinate multiple\naligned channels such as slides, subtitles, speech, and human talker. To\naddress these challenges, we introduce PaperTalker, the first benchmark of 101\nresearch papers paired with author-created presentation videos, slides, and\nspeaker metadata. We further design four tailored evaluation metrics--Meta\nSimilarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos\nconvey the paper's information to the audience. Building on this foundation, we\npropose PaperTalker, the first multi-agent framework for academic presentation\nvideo generation. It integrates slide generation with effective layout\nrefinement by a novel effective tree search visual choice, cursor grounding,\nsubtitling, speech synthesis, and talking-head rendering, while parallelizing\nslide-wise generation for efficiency. Experiments on Paper2Video demonstrate\nthat the presentation videos produced by our approach are more faithful and\ninformative than existing baselines, establishing a practical step toward\nautomated and ready-to-use academic video generation. Our dataset, agent, and\ncode are available at https://github.com/showlab/Paper2Video.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.05096v1",
    "published_at": "2025-10-06T17:58:02",
    "created_at": "2025-10-07T09:00:39.870987",
    "key_finding": "**** papertalker enables the automatic generation of academic presentation videos from research papers, outperforming existing methods in faithfulness and informativeness.",
    "methodology": "",
    "implications": "**methodology:** the framework integrates slide generation, layout refinement, subtitling, speech synthesis, and talking-head rendering in a multi-agent architecture, utilizing a custom benchmark of 101 research papers linked to author-created videos."
  },
  {
    "id": 624,
    "title": "optimizing alignment in large reasoning models",
    "body": "Large reasoning models (LRMs) generate intermediate reasoning traces before\nproducing final answers, yielding strong gains on multi-step and mathematical\ntasks. Yet aligning LRMs with human preferences, a crucial prerequisite for\nmodel deployment, remains underexplored. The statistically correct objective\nfor preference alignment requires marginalizing over reasoning traces, but this\ncomputation is intractable in practice. A common workaround optimizes a single\nsampled trajectory, which introduces substantial gradient variance from\nstochastic trace sampling. To address this challenge, we frame preference\noptimization for LRMs through the lens of the bias--variance trade-off and\npropose Bias--Variance Optimized Preference Optimization (BVPO), a simple,\ndrop-in method that mixes two gradient estimators: a high-variance trace-based\nestimator and a low-variance empty-trace estimator obtained by disabling\nreasoning trace generation. Our theory shows that BVPO strictly reduces\ntrace-induced variance for any nontrivial mixture, provides a closed-form\nchoice of the mixing weight that minimizes mean-squared error relative to the\ntrue marginal gradient, and under standard smoothness and step-size conditions,\ntightens classical convergence bounds for stochastic gradient descent.\nEmpirically, BVPO improves alignment over the best baseline by up to 7.8 points\non AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on\ngeneral conversational data, BVPO also boosts reasoning performance for base\nmodels by up to 4.0 points on the average of six math reasoning benchmarks.\nThese results identify variance from trace sampling as a key bottleneck and\ndemonstrate that directly optimizing the bias--variance trade-off yields more\nstable training and stronger overall performance.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.05095v1",
    "published_at": "2025-10-06T17:58:01",
    "created_at": "2025-10-07T09:00:45.692742",
    "key_finding": "the proposed bias-variance optimized preference optimization (bvpo) method significantly enhances the alignment of large reasoning models (lrms) with human preferences by reducing gradient variance caused by tracing noise.",
    "methodology": "bvpo combines a high-variance trace-based gradient estimator with a low-variance empty-trace estimator to optimize the bias-variance trade-off, theoretically ensuring reduced variance and improved convergence properties.",
    "implications": "this approach not only leads to improved alignment scores on benchmarks like alpacaeval~2 and arena-hard but also enhances reasoning performance on mathematical tasks, highlighting the importance of addressing gradient variance in the training of lrms."
  },
  {
    "id": 625,
    "title": "Learning to Interpret Weight Differences in Language Models",
    "body": "Finetuning (pretrained) language models is a standard approach for updating\ntheir internal parametric knowledge and specializing them to new tasks and\ndomains. However, the corresponding model weight changes (\"weight diffs\") are\nnot generally interpretable. While inspecting the finetuning dataset can give a\nsense of how the model might have changed, these datasets are often not\npublicly available or are too large to work with directly. Towards the goal of\ncomprehensively understanding weight diffs in natural language, we introduce\nDiff Interpretation Tuning (DIT), a method that trains models to describe their\nown finetuning-induced modifications. Our approach uses synthetic, labeled\nweight diffs to train a DIT adapter, which can be applied to a compatible\nfinetuned model to make it describe how it has changed. We demonstrate in two\nproof-of-concept settings (reporting hidden behaviors and summarizing finetuned\nknowledge) that our method enables models to describe their finetuning-induced\nmodifications using accurate natural language descriptions.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.05092v1",
    "published_at": "2025-10-06T17:57:23",
    "created_at": "2025-10-07T09:00:48.526986",
    "key_finding": "the authors introduce diff interpretation tuning (dit), a method that enables language models to articulate the changes in their weights resulting from finetuning in natural language.",
    "methodology": "dit employs synthetic, labeled weight differences to train an adapter that instructs the model on how to describe its finetuning modifications.",
    "implications": "this approach enhances the interpretability of language models, providing insights into finetuning effects and facilitating better understanding of model behavior and knowledge adaptation in various applications."
  },
  {
    "id": 626,
    "title": "Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models",
    "body": "Diffusion large language models (dLLMs) have recently emerged as a promising\nalternative to autoregressive (AR) models, offering advantages such as\naccelerated parallel decoding and bidirectional context modeling. However, the\nvanilla decoding strategy in discrete dLLMs suffers from a critical limitation:\nonce a token is accepted, it can no longer be revised in subsequent steps. As a\nresult, early mistakes persist across iterations, harming both intermediate\npredictions and final output quality. To address this issue, we propose\nTolerator (Token-Level Cross-Validation Refinement), a training-free decoding\nstrategy that leverages cross-validation among predicted tokens. Unlike\nexisting methods that follow a single progressive unmasking procedure,\nTolerator introduces a two-stage process: (i) sequence fill-up and (ii)\niterative refinement by remasking and decoding a subset of tokens while\ntreating the remaining as context. This design enables previously accepted\ntokens to be reconsidered and corrected when necessary, leading to more\nreliable diffusion decoding outputs. We evaluate Tolerator on five standard\nbenchmarks covering language understanding, code generation, and mathematics.\nExperiments show that our method achieves consistent improvements over the\nbaselines under the same computational budget. These findings suggest that\ndecoding algorithms are crucial to realizing the full potential of diffusion\nlarge language models. Code and data are publicly available.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.05090v1",
    "published_at": "2025-10-06T17:56:46",
    "created_at": "2025-10-07T09:00:51.982395",
    "key_finding": "the proposed tolerator decoding strategy significantly enhances the quality of outputs from diffusion large language models by allowing for token reconsideration and correction.",
    "methodology": "tolerator employs a two-stage process, consisting of sequence fill-up and iterative refinement through remasking, to enable token-level cross-validation without requiring additional training.",
    "implications": "this research demonstrates the importance of decoding strategies in optimizing the performance of diffusion large language models, highlighting their practical applications in language understanding, code generation, and mathematics."
  },
  {
    "id": 617,
    "title": "Reward Models are Metrics in a Trench Coat",
    "body": "The emergence of reinforcement learning in post-training of large language\nmodels has sparked significant interest in reward models. Reward models assess\nthe quality of sampled model outputs to generate training signals. This task is\nalso performed by evaluation metrics that monitor the performance of an AI\nmodel. We find that the two research areas are mostly separate, leading to\nredundant terminology and repeated pitfalls. Common challenges include\nsusceptibility to spurious correlations, impact on downstream reward hacking,\nmethods to improve data quality, and approaches to meta-evaluation. Our\nposition paper argues that a closer collaboration between the fields can help\novercome these issues. To that end, we show how metrics outperform reward\nmodels on specific tasks and provide an extensive survey of the two areas.\nGrounded in this survey, we point to multiple research topics in which closer\nalignment can improve reward models and metrics in areas such as preference\nelicitation methods, avoidance of spurious correlations and reward hacking, and\ncalibration-aware meta-evaluation.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.03231v1",
    "published_at": "2025-10-03T17:59:44",
    "created_at": "2025-10-06T09:00:35.107198",
    "key_finding": "the paper highlights significant overlap and mutual challenges between reward models and evaluation metrics in reinforcement learning, advocating for increased collaboration to mitigate issues like spurious correlations and reward hacking.",
    "methodology": "it employs a comprehensive survey to compare performance and identify key areas of synergy between reward models and metrics, demonstrating metrics' superiority in specific tasks.",
    "implications": "by fostering collaboration between these fields, the research suggests that advancements in areas like preference elicitation and meta-evaluation can enhance both reward models and metrics, leading to more robust ai evaluation frameworks."
  },
  {
    "id": 618,
    "title": "Improving GUI Grounding with Explicit Position-to-Coordinate Mapping",
    "body": "GUI grounding, the task of mapping natural-language instructions to pixel\ncoordinates, is crucial for autonomous agents, yet remains difficult for\ncurrent VLMs. The core bottleneck is reliable patch-to-pixel mapping, which\nbreaks when extrapolating to high-resolution displays unseen during training.\nCurrent approaches generate coordinates as text tokens directly from visual\nfeatures, forcing the model to infer complex position-to-pixel mappings\nimplicitly; as a result, accuracy degrades and failures proliferate on new\nresolutions. We address this with two complementary innovations. First, RULER\ntokens serve as explicit coordinate markers, letting the model reference\npositions similar to gridlines on a map and adjust rather than generate\ncoordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial\nencoding by ensuring that width and height dimensions are represented equally,\naddressing the asymmetry of standard positional schemes. Experiments on\nScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in\ngrounding accuracy, with the largest improvements on high-resolution\ninterfaces. By providing explicit spatial guidance rather than relying on\nimplicit learning, our approach enables more reliable GUI automation across\ndiverse resolutions and platforms.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.03230v1",
    "published_at": "2025-10-03T17:59:34",
    "created_at": "2025-10-06T09:00:38.003138",
    "key_finding": "the proposed ruler tokens and interleaved mrope (i-mrope) techniques significantly enhance gui grounding accuracy, particularly for high-resolution displays.",
    "methodology": "the study introduces explicit coordinate mapping through ruler tokens alongside a balanced spatial encoding approach (i-mrope) to improve the model's ability to accurately translate natural language instructions into pixel coordinates.",
    "implications": "this research provides a more reliable framework for gui automation in various resolutions and platforms, addressing the limitations of current visual language models (vlms) and reducing the frequency of failures in complex environments."
  },
  {
    "id": 619,
    "title": "A fast non-reversible sampler for Bayesian finite mixture models",
    "body": "Finite mixtures are a cornerstone of Bayesian modelling, and it is well-known\nthat sampling from the resulting posterior distribution can be a hard task. In\nparticular, popular reversible Markov chain Monte Carlo schemes are often slow\nto converge when the number of observations $n$ is large. In this paper we\nintroduce a novel and simple non-reversible sampling scheme for Bayesian finite\nmixture models, which is shown to drastically outperform classical samplers in\nmany scenarios of interest, especially during convergence phase and when\ncomponents in the mixture have non-negligible overlap. At the theoretical\nlevel, we show that the performance of the proposed non-reversible scheme\ncannot be worse than the standard one, in terms of asymptotic variance, by more\nthan a factor of four; and we provide a scaling limit analysis suggesting that\nthe non-reversible sampler can reduce the convergence time from O$(n^2)$ to\nO$(n)$. We also discuss why the statistical features of mixture models make\nthem an ideal case for the use of non-reversible discrete samplers.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.03226v1",
    "published_at": "2025-10-03T17:57:44",
    "created_at": "2025-10-06T09:00:40.992399",
    "key_finding": "the proposed non-reversible sampler for bayesian finite mixture models significantly improves convergence speed compared to traditional reversible markov chain monte carlo methods, particularly in cases of high component overlap.",
    "methodology": "the paper develops a straightforward non-reversible sampling scheme and provides theoretical guarantees demonstrating that its asymptotic variance is at most four times worse than that of standard samplers, alongside a scaling limit analysis indicating a reduction in convergence time from o$(n^2)$ to o$(n)$.",
    "implications": "this research suggests that non-reversible sampling methods could be more effective for complex bayesian models, potentially leading to faster and more accurate inference in real-world applications where finite mixtures are commonly employed."
  },
  {
    "id": 620,
    "title": "Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles",
    "body": "We propose a test-time defense mechanism against adversarial attacks:\nimperceptible image perturbations that significantly alter the predictions of a\nmodel. Unlike existing methods that rely on feature filtering or smoothing,\nwhich can lead to information loss, we propose to \"combat noise with noise\" by\nleveraging stochastic resonance to enhance robustness while minimizing\ninformation loss. Our approach introduces small translational perturbations to\nthe input image, aligns the transformed feature embeddings, and aggregates them\nbefore mapping back to the original reference image. This can be expressed in a\nclosed-form formula, which can be deployed on diverse existing network\narchitectures without introducing additional network modules or fine-tuning for\nspecific attack types. The resulting method is entirely training-free,\narchitecture-agnostic, and attack-agnostic. Empirical results show\nstate-of-the-art robustness on image classification and, for the first time,\nestablish a generic test-time defense for dense prediction tasks, including\nstereo matching and optical flow, highlighting the method's versatility and\npracticality. Specifically, relative to clean (unperturbed) performance, our\nmethod recovers up to 68.1% of the accuracy loss on image classification, 71.9%\non stereo matching, and 29.2% on optical flow under various types of\nadversarial attacks.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.03224v1",
    "published_at": "2025-10-03T17:57:25",
    "created_at": "2025-10-06T09:00:44.346460",
    "key_finding": "the proposed test-time defense mechanism employing stochastic resonance significantly mitigates accuracy loss in various tasks under adversarial attacks, demonstrating state-of-the-art robustness.",
    "methodology": "by applying small translational perturbations to input images and aggregating the transformed feature embeddings, the method enhances robustness without requiring additional network modules or fine-tuning for specific attacks.",
    "implications": "this training-free, architecture-agnostic, and attack-agnostic approach not only improves image classification performance but also expands generic defense capabilities to dense prediction tasks like stereo matching and optical flow."
  },
  {
    "id": 621,
    "title": "Self-Anchor: Large Language Model Reasoning via Step-by-step Attention Alignment",
    "body": "To solve complex reasoning tasks for Large Language Models (LLMs),\nprompting-based methods offer a lightweight alternative to fine-tuning and\nreinforcement learning. However, as reasoning chains extend, critical\nintermediate steps and the original prompt will be buried in the context,\nreceiving insufficient attention and leading to errors. In this paper, we\npropose Self-Anchor, a novel pipeline that leverages the inherent structure of\nreasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories\ninto structured plans and automatically aligns the model's attention to the\nmost relevant inference steps, allowing the model to maintain focus throughout\ngeneration. Our experiment shows that Self-Anchor outperforms SOTA prompting\nmethods across six benchmarks. Notably, Self-Anchor significantly reduces the\nperformance gap between ``non-reasoning'' models and specialized reasoning\nmodels, with the potential to enable most LLMs to tackle complex reasoning\ntasks without retraining.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.03223v1",
    "published_at": "2025-10-03T17:56:33",
    "created_at": "2025-10-06T09:00:46.908734",
    "key_finding": "self-anchor enhances the reasoning capabilities of large language models by improving attention alignment during complex tasks, resulting in superior performance compared to state-of-the-art prompting methods.",
    "methodology": "the approach involves decomposing reasoning trajectories into structured plans and automatically guiding the model's attention to the most relevant steps of inference.",
    "implications": "this novel framework significantly narrows the performance gap between standard and specialized reasoning models, enabling broader applicability of llms to complex reasoning tasks without the need for retraining."
  },
  {
    "id": 612,
    "title": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation",
    "body": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.02312v1",
    "published_at": "2025-10-02T17:59:51",
    "created_at": "2025-10-03T09:00:34.791274",
    "key_finding": "kava effectively distills knowledge from a compressed kv-cache into a latent-reasoning model, improving performance on complex reasoning tasks while maintaining computational efficiency.",
    "methodology": "the framework utilizes self-distillation to align stepwise kv trajectories from a teacher model with its compressed cache, which serves as a rich supervisory signal without direct token correspondence.",
    "implications": "this approach establishes compressed kv-cache distillation as a viable method for enhancing latent reasoning in large language models, combining the strengths of explicit chain-of-thought reasoning with the operational efficiency required for practical deployment."
  },
  {
    "id": 613,
    "title": "Inferring Dynamic Physical Properties from Video Foundation Models",
    "body": "We study the task of predicting dynamic physical properties from videos. More\nspecifically, we consider physical properties that require temporal information\nto be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,\nand dynamic friction of an object sliding on a surface. To this end, we make\nthe following contributions: (i) We collect a new video dataset for each\nphysical property, consisting of synthetic training and testing splits, as well\nas a real split for real world evaluation. (ii) We explore three ways to infer\nthe physical property from videos: (a) an oracle method where we supply the\nvisual cues that intrinsically reflect the property using classical computer\nvision techniques; (b) a simple read out mechanism using a visual prompt and\ntrainable prompt vector for cross-attention on pre-trained video generative and\nself-supervised models; and (c) prompt strategies for Multi-modal Large\nLanguage Models (MLLMs). (iii) We show that video foundation models trained in\na generative or self-supervised manner achieve a similar performance, though\nbehind that of the oracle, and MLLMs are currently inferior to the other\nmodels, though their performance can be improved through suitable prompting.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.02311v1",
    "published_at": "2025-10-02T17:59:50",
    "created_at": "2025-10-03T09:00:38.573548",
    "key_finding": "**** the study successfully infers dynamic physical properties from videos, revealing that video foundation models can approximate oracle performance while mllms underperform but can be enhanced with better prompting strategies.",
    "methodology": "**** a new video dataset was created for synthetic and real evaluations, with three inference methods explored: classical computer vision for oracle performance, trainable prompt vectors using cross-attention with video generative models, and prompt strategies for mllms.",
    "implications": "**** this research highlights the potential of leveraging video foundation models for understanding physical properties, paving the way for advancements in applications such as robotics, animation, and material science."
  },
  {
    "id": 614,
    "title": "Robust Tangent Space Estimation via Laplacian Eigenvector Gradient Orthogonalization",
    "body": "Estimating the tangent spaces of a data manifold is a fundamental problem in\ndata analysis. The standard approach, Local Principal Component Analysis\n(LPCA), struggles in high-noise settings due to a critical trade-off in\nchoosing the neighborhood size. Selecting an optimal size requires prior\nknowledge of the geometric and noise characteristics of the data that are often\nunavailable. In this paper, we propose a spectral method, Laplacian Eigenvector\nGradient Orthogonalization (LEGO), that utilizes the global structure of the\ndata to guide local tangent space estimation. Instead of relying solely on\nlocal neighborhoods, LEGO estimates the tangent space at each data point by\northogonalizing the gradients of low-frequency eigenvectors of the graph\nLaplacian. We provide two theoretical justifications of our method. First, a\ndifferential geometric analysis on a tubular neighborhood of a manifold shows\nthat gradients of the low-frequency Laplacian eigenfunctions of the tube align\nclosely with the manifold's tangent bundle, while an eigenfunction with high\ngradient in directions orthogonal to the manifold lie deeper in the spectrum.\nSecond, a random matrix theoretic analysis also demonstrates that low-frequency\neigenvectors are robust to sub-Gaussian noise. Through comprehensive\nexperiments, we demonstrate that LEGO yields tangent space estimates that are\nsignificantly more robust to noise than those from LPCA, resulting in marked\nimprovements in downstream tasks such as manifold learning, boundary detection,\nand local intrinsic dimension estimation.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.02308v1",
    "published_at": "2025-10-02T17:59:45",
    "created_at": "2025-10-03T09:00:42.590105",
    "key_finding": "the proposed method, laplacian eigenvector gradient orthogonalization (lego), provides more robust tangent space estimates in high-noise settings compared to standard local principal component analysis (lpca).",
    "methodology": "lego leverages the global structure of the data by orthogonalizing the gradients of low-frequency eigenvectors of the graph laplacian, rather than relying solely on local neighborhoods for tangent space estimation.",
    "implications": "this approach not only enhances the robustness of tangent space estimation but also improves performance in manifold learning, boundary detection, and local intrinsic dimension estimation under challenging noise conditions."
  },
  {
    "id": 615,
    "title": "NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation",
    "body": "Text-to-image diffusion models trained on a fixed set of resolutions often\nfail to generalize, even when asked to generate images at lower resolutions\nthan those seen during training. High-resolution text-to-image generators are\ncurrently unable to easily offer an out-of-the-box budget-efficient alternative\nto their users who might not need high-resolution images. We identify a key\ntechnical insight in diffusion models that when addressed can help tackle this\nlimitation: Noise schedulers have unequal perceptual effects across\nresolutions. The same level of noise removes disproportionately more signal\nfrom lower-resolution images than from high-resolution images, leading to a\ntrain-test mismatch. We propose NoiseShift, a training-free method that\nrecalibrates the noise level of the denoiser conditioned on resolution size.\nNoiseShift requires no changes to model architecture or sampling schedule and\nis compatible with existing models. When applied to Stable Diffusion 3, Stable\nDiffusion 3.5, and Flux-Dev, quality at low resolutions is significantly\nimproved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and\nFlux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by\n10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results\ndemonstrate the effectiveness of NoiseShift in mitigating resolution-dependent\nartifacts and enhancing the quality of low-resolution image generation.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.02307v1",
    "published_at": "2025-10-02T17:59:43",
    "created_at": "2025-10-03T09:00:45.449075",
    "key_finding": "noiseshift significantly enhances low-resolution image generation by recalibrating the noise level in diffusion models, leading to improved fid scores across various models and datasets.",
    "methodology": "the method operates without altering model architecture or sampling schedules, implementing a training-free approach that adjusts the denoiser's noise levels based on resolution size.",
    "implications": "by addressing resolution-specific noise challenges, noiseshift provides a practical solution for users requiring high-quality low-resolution images, bridging the gap in performance between high and low resolutions in text-to-image diffusion models."
  },
  {
    "id": 616,
    "title": "Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is Geometry Adaptive",
    "body": "Diffusion models have achieved state-of-the-art performance, demonstrating\nremarkable generalisation capabilities across diverse domains. However, the\nmechanisms underpinning these strong capabilities remain only partially\nunderstood. A leading conjecture, based on the manifold hypothesis, attributes\nthis success to their ability to adapt to low-dimensional geometric structure\nwithin the data. This work provides evidence for this conjecture, focusing on\nhow such phenomena could result from the formulation of the learning problem\nthrough score matching. We inspect the role of implicit regularisation by\ninvestigating the effect of smoothing minimisers of the empirical score\nmatching objective. Our theoretical and empirical results confirm that\nsmoothing the score function -- or equivalently, smoothing in the log-density\ndomain -- produces smoothing tangential to the data manifold. In addition, we\nshow that the manifold along which the diffusion model generalises can be\ncontrolled by choosing an appropriate smoothing.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2510.02305v1",
    "published_at": "2025-10-02T17:59:39",
    "created_at": "2025-10-03T09:00:49.112415",
    "key_finding": "this research validates the manifold hypothesis by demonstrating that log-domain smoothing in diffusion models aligns with the data's underlying geometric structure, enhancing generalization.",
    "methodology": "the study employs both theoretical analysis and empirical experimentation to explore the effects of smoothing on the empirical score matching objective, revealing its relationship to the data manifold.",
    "implications": "the results suggest that by selecting appropriate smoothing techniques, one can effectively control and enhance the manifold along which diffusion models generalize, potentially improving their performance across various applications."
  },
  {
    "id": 607,
    "title": "Stitch: Training-Free Position Control in Multimodal Diffusion Transformers",
    "body": "Text-to-Image (T2I) generation models have advanced rapidly in recent years,\nbut accurately capturing spatial relationships like \"above\" or \"to the right\nof\" poses a persistent challenge. Earlier methods improved spatial relationship\nfollowing with external position control. However, as architectures evolved to\nenhance image quality, these techniques became incompatible with modern models.\nWe propose Stitch, a training-free method for incorporating external position\ncontrol into Multi-Modal Diffusion Transformers (MMDiT) via\nautomatically-generated bounding boxes. Stitch produces images that are both\nspatially accurate and visually appealing by generating individual objects\nwithin designated bounding boxes and seamlessly stitching them together. We\nfind that targeted attention heads capture the information necessary to isolate\nand cut out individual objects mid-generation, without needing to fully\ncomplete the image. We evaluate Stitch on PosEval, our benchmark for\nposition-based T2I generation. Featuring five new tasks that extend the concept\nof Position beyond the basic GenEval task, PosEval demonstrates that even top\nmodels still have significant room for improvement in position-based\ngeneration. Tested on Qwen-Image, FLUX, and SD3.5, Stitch consistently enhances\nbase models, even improving FLUX by 218% on GenEval's Position task and by 206%\non PosEval. Stitch achieves state-of-the-art results with Qwen-Image on\nPosEval, improving over previous models by 54%, all accomplished while\nintegrating position control into leading models training-free. Code is\navailable at https://github.com/ExplainableML/Stitch.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.26644v1",
    "published_at": "2025-09-30T17:59:51",
    "created_at": "2025-10-01T09:00:36.460981",
    "key_finding": "stitch is a training-free method that enhances text-to-image generation by effectively integrating external position control into multimodal diffusion transformers, significantly improving spatial accuracy and visual quality.",
    "methodology": "stitch employs automatically-generated bounding boxes and targeted attention heads to isolate and generate individual objects mid-generation, enabling seamless composition of images without full completion.",
    "implications": "this approach not only sets a new benchmark for position-based text-to-image generation, as demonstrated through substantial performance improvements on multiple models, but also highlights the potential for enhancing existing architectures without the need for retraining."
  },
  {
    "id": 608,
    "title": "Convergence and Divergence of Language Models under Different Random Seeds",
    "body": "In this paper, we investigate the convergence of language models (LMs)\ntrained under different random seeds, measuring convergence as the expected\nper-token Kullback--Leibler (KL) divergence across seeds. By comparing LM\nconvergence as a function of model size and training checkpoint, we identify a\nfour-phase convergence pattern: (i) an initial uniform phase, (ii) a\nsharp-convergence phase, (iii) a sharp-divergence phase, and (iv) a\nslow-reconvergence phase. Further, we observe that larger models reconverge\nfaster in later training stages, while smaller models never actually\nreconverge; these results suggest that a certain model size may be necessary to\nlearn stable distributions. Restricting our analysis to specific token\nfrequencies or part-of-speech (PoS) tags further reveals that convergence is\nuneven across linguistic categories: frequent tokens and function words\nconverge faster and more reliably than their counterparts (infrequent tokens\nand content words). Overall, our findings highlight factors that influence the\nstability of the learned distributions in model training.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.26643v1",
    "published_at": "2025-09-30T17:59:50",
    "created_at": "2025-10-01T09:00:39.640327",
    "key_finding": "**** the study uncovers a four-phase convergence pattern in language models trained with different random seeds, revealing that model size significantly influences the stability and speed of convergence, particularly across different token types.",
    "methodology": "**** the authors measured the expected per-token kullback-leibler divergence across seeds while analyzing convergence patterns based on model size and training checkpoints.",
    "implications": "**** these results suggest that larger models are more capable of learning stable distributions and indicate that linguistic factors, like token frequency and part-of-speech, affect convergence reliability, which has important implications for model training strategies."
  },
  {
    "id": 609,
    "title": "SPATA: Systematic Pattern Analysis for Detailed and Transparent Data Cards",
    "body": "Due to the susceptibility of Artificial Intelligence (AI) to data\nperturbations and adversarial examples, it is crucial to perform a thorough\nrobustness evaluation before any Machine Learning (ML) model is deployed.\nHowever, examining a model's decision boundaries and identifying potential\nvulnerabilities typically requires access to the training and testing datasets,\nwhich may pose risks to data privacy and confidentiality. To improve\ntransparency in organizations that handle confidential data or manage critical\ninfrastructure, it is essential to allow external verification and validation\nof AI without the disclosure of private datasets. This paper presents\nSystematic Pattern Analysis (SPATA), a deterministic method that converts any\ntabular dataset to a domain-independent representation of its statistical\npatterns, to provide more detailed and transparent data cards. SPATA computes\nthe projection of each data instance into a discrete space where they can be\nanalyzed and compared, without risking data leakage. These projected datasets\ncan be reliably used for the evaluation of how different features affect ML\nmodel robustness and for the generation of interpretable explanations of their\nbehavior, contributing to more trustworthy AI.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.26640v1",
    "published_at": "2025-09-30T17:59:45",
    "created_at": "2025-10-01T09:00:41.949712",
    "key_finding": "spata enables detailed and transparent evaluation of ai model robustness without compromising the privacy of underlying datasets.",
    "methodology": "the approach transforms tabular datasets into a secure, domain-independent representation of statistical patterns by projecting data instances into a discrete space.",
    "implications": "this method facilitates external validation of ai systems, promoting trustworthiness and enabling organizations to assess feature impacts on model robustness without revealing sensitive data."
  },
  {
    "id": 610,
    "title": "AccidentBench: Benchmarking Multimodal Understanding and Reasoning in Vehicle Accidents and Beyond",
    "body": "Rapid advances in multimodal models demand benchmarks that rigorously\nevaluate understanding and reasoning in safety-critical, dynamic real-world\nsettings. We present AccidentBench, a large-scale benchmark that combines\nvehicle accident scenarios with Beyond domains, safety-critical settings in air\nand water that emphasize spatial and temporal reasoning (e.g., navigation,\norientation, multi-vehicle motion). The benchmark contains approximately 2000\nvideos and over 19000 human-annotated question--answer pairs spanning multiple\nvideo lengths (short/medium/long) and difficulty levels (easy/medium/hard).\nTasks systematically probe core capabilities: temporal, spatial, and intent\nunderstanding and reasoning. By unifying accident-centric traffic scenes with\nbroader safety-critical scenarios in air and water, AccidentBench offers a\ncomprehensive, physically grounded testbed for evaluating models under\nreal-world variability. Evaluations of state-of-the-art models (e.g.,\nGemini-2.5 Pro and GPT-5) show that even the strongest models achieve only\nabout 18% accuracy on the hardest tasks and longest videos, revealing\nsubstantial gaps in real-world temporal, spatial, and intent reasoning.\nAccidentBench is designed to expose these critical gaps and drive the\ndevelopment of multimodal models that are safer, more robust, and better\naligned with real-world safety-critical challenges. The code and dataset are\navailable at: https://github.com/SafeRL-Lab/AccidentBench",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.26636v1",
    "published_at": "2025-09-30T17:59:13",
    "created_at": "2025-10-01T09:00:45.324978",
    "key_finding": "**** accidentbench reveals that state-of-the-art multimodal models exhibit significant deficiencies, achieving only about 18% accuracy in the most challenging tasks related to vehicle accidents and other safety-critical scenarios.",
    "methodology": "",
    "implications": "**methodology:** the benchmark comprises approximately 2000 videos accompanied by over 19,000 human-annotated question-answer pairs, systematically testing capabilities in temporal, spatial, and intent understanding across varying video lengths and difficulty levels."
  },
  {
    "id": 611,
    "title": "OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction",
    "body": "A dominant paradigm for teaching humanoid robots complex skills is to\nretarget human motions as kinematic references to train reinforcement learning\n(RL) policies. However, existing retargeting pipelines often struggle with the\nsignificant embodiment gap between humans and robots, producing physically\nimplausible artifacts like foot-skating and penetration. More importantly,\ncommon retargeting methods neglect the rich human-object and human-environment\ninteractions essential for expressive locomotion and loco-manipulation. To\naddress this, we introduce OmniRetarget, an interaction-preserving data\ngeneration engine based on an interaction mesh that explicitly models and\npreserves the crucial spatial and contact relationships between an agent, the\nterrain, and manipulated objects. By minimizing the Laplacian deformation\nbetween the human and robot meshes while enforcing kinematic constraints,\nOmniRetarget generates kinematically feasible trajectories. Moreover,\npreserving task-relevant interactions enables efficient data augmentation, from\na single demonstration to different robot embodiments, terrains, and object\nconfigurations. We comprehensively evaluate OmniRetarget by retargeting motions\nfrom OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour\ntrajectories that achieve better kinematic constraint satisfaction and contact\npreservation than widely used baselines. Such high-quality data enables\nproprioceptive RL policies to successfully execute long-horizon (up to 30\nseconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained\nwith only 5 reward terms and simple domain randomization shared by all tasks,\nwithout any learning curriculum.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.26633v1",
    "published_at": "2025-09-30T17:59:02",
    "created_at": "2025-10-01T09:00:47.954626",
    "key_finding": "omniretarget effectively generates kinematically feasible trajectories that preserve crucial interactions in humanoid loco-manipulation, outperforming existing methods.",
    "methodology": "the system utilizes an interaction mesh to model spatial and contact relationships, minimizing deformation between human and robot meshes while enforcing kinematic constraints, and supports data augmentation across various scenarios.",
    "implications": "this innovative approach allows proprioceptive reinforcement learning policies to demonstrate complex parkour and loco-manipulation skills on a humanoid robot with minimal reward structure, significantly enhancing the learning efficiency and performance of robotic systems."
  },
  {
    "id": 602,
    "title": "UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following",
    "body": "Shaping powerful LLMs to be beneficial and safe is central to AI alignment.\nWe argue that post-training alignment is fundamentally a unified Preference\nLearning problem, involving two modalities: demonstrated preferences (e.g.,\nSupervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement\nLearning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due\nto a critical distributional mismatch: SFT uses static expert data, but as the\npolicy evolves, its generation distribution drifts, making SFT knowledge\nbrittle. Subsequent RL then explores without direct access to the rich,\nground-truth knowledge in expert demonstrations, leading to inefficient,\nungrounded updates. This separation prevents mutual regularization between data\nsources. To address this, we reframe alignment as a constrained optimization\nproblem and propose Unified Adversarial Preference Learning (UniAPL),a novel\nframework that dynamically aligns the policy's distribution with the expert's.\nUniAPL implements a single-stage unified training objective, jointly learning\nfrom mixed batches of SFT and preference data. In every gradient step, dense\nexpert demonstrations directly ground and regularize online exploration,\ninherently resolving distributional mismatch and maximizing data synergy.We\nevaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507\nas the teacher. Our models match or exceed strong GRPO baselines: +5.77% on\nQwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the\nteacher. Analyses of response length and log-probability distributions confirm\nthat UniAPL outputs closely mimic expert demonstrations, achieving both\nstronger performance and better behavioral alignment.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.25148v1",
    "published_at": "2025-09-29T17:53:09",
    "created_at": "2025-09-30T09:01:08.307244",
    "key_finding": "**** the uniapl framework significantly enhances instruction-following in large language models by effectively aligning policy distributions with expert preferences, outperforming traditional methods.",
    "methodology": "**** uniapl utilizes a unified training objective that concurrently learns from both supervised fine-tuning and comparative preferences, optimizing model alignment through a constrained optimization approach to resolve distributional mismatches.",
    "implications": "**** this improved alignment strategy not only boosts performance but also ensures that model outputs are more closely aligned with expert behavior, advancing the safety and utility of ai systems."
  },
  {
    "id": 603,
    "title": "Fast Feature Field ($\\text{F}^3$): A Predictive Representation of Events",
    "body": "This paper develops a mathematical argument and algorithms for building\nrepresentations of data from event-based cameras, that we call Fast Feature\nField ($\\text{F}^3$). We learn this representation by predicting future events\nfrom past events and show that it preserves scene structure and motion\ninformation. $\\text{F}^3$ exploits the sparsity of event data and is robust to\nnoise and variations in event rates. It can be computed efficiently using ideas\nfrom multi-resolution hash encoding and deep sets - achieving 120 Hz at HD and\n440 Hz at VGA resolutions. $\\text{F}^3$ represents events within a contiguous\nspatiotemporal volume as a multi-channel image, enabling a range of downstream\ntasks. We obtain state-of-the-art performance on optical flow estimation,\nsemantic segmentation, and monocular metric depth estimation, on data from\nthree robotic platforms (a car, a quadruped robot and a flying platform),\nacross different lighting conditions (daytime, nighttime), environments\n(indoors, outdoors, urban, as well as off-road) and dynamic vision sensors\n(resolutions and event rates). Our implementations can predict these tasks at\n25-75 Hz at HD resolution.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.25146v1",
    "published_at": "2025-09-29T17:52:31",
    "created_at": "2025-09-30T09:01:12.987015",
    "key_finding": "**** the fast feature field ($\\text{f}^3$) method provides an efficient and robust representation of event-based camera data that preserves essential scene structure and motion information for various tasks.",
    "methodology": "",
    "implications": "**methodology:** $\\text{f}^3$ is learned through a predictive model that forecasts future events based on past data, leveraging the sparsity of event representations and utilizing multi-resolution hash encoding and deep sets for efficient computation."
  },
  {
    "id": 604,
    "title": "turning unpaired data into high-fidelity pairs for text generation",
    "body": "We present Paired by the Teacher (PbT), a two-stage teacher-student pipeline\nthat synthesizes accurate input-output pairs without human labels or parallel\ndata. In many low-resource natural language generation (NLG) scenarios,\npractitioners may have only raw outputs, like highlights, recaps, or questions,\nor only raw inputs, such as articles, dialogues, or paragraphs, but seldom\nboth. This mismatch forces small models to learn from very few examples or rely\non costly, broad-scope synthetic examples produced by large LLMs. PbT addresses\nthis by asking a teacher LLM to compress each unpaired example into a concise\nintermediate representation (IR), and training a student to reconstruct inputs\nfrom IRs. This enables outputs to be paired with student-generated inputs,\nyielding high-quality synthetic data. We evaluate PbT on five\nbenchmarks-document summarization (XSum, CNNDM), dialogue summarization\n(SAMSum, DialogSum), and question generation (SQuAD)-as well as an unpaired\nsetting on SwitchBoard (paired with DialogSum summaries). An 8B student trained\nonly on PbT data outperforms models trained on 70 B teacher-generated corpora\nand other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated\npairs and closing 82% of the oracle gap at one-third the annotation cost of\ndirect synthesis. Human evaluation on SwitchBoard further confirms that only\nPbT produces concise, faithful summaries aligned with the target style,\nhighlighting its advantage of generating in-domain sources that avoid the\nmismatch, limiting direct synthesis.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.25144v1",
    "published_at": "2025-09-29T17:51:55",
    "created_at": "2025-09-30T09:01:18.151940",
    "key_finding": "the paired by the teacher (pbt) method generates high-quality input-output pairs for low-resource text generation, significantly enhancing performance while reducing annotation costs.",
    "methodology": "pbt utilizes a two-stage teacher-student pipeline, where a teacher llm compresses unpaired examples into concise intermediate representations, enabling a student model to reconstruct inputs and generate paired synthetic outputs.",
    "implications": "this approach effectively mitigates the challenge of working with sparse data, allowing small models to perform competitively against larger, extensively trained systems, while preserving the fidelity and style of the desired outputs."
  },
  {
    "id": 605,
    "title": "Visual serial processing deficits explain divergences in human and VLM reasoning",
    "body": "Why do Vision Language Models (VLMs), despite success on standard benchmarks,\noften fail to match human performance on surprisingly simple visual reasoning\ntasks? While the underlying computational principles are still debated, we\nhypothesize that a crucial factor is a deficit in visually-grounded serial\nprocessing. To test this hypothesis, we compared human and VLM performance\nacross tasks designed to vary serial processing demands in three distinct\ndomains: geometric reasoning, perceptual enumeration, and mental rotation.\nTasks within each domain varied serial processing load by manipulating factors\nsuch as geometric concept complexity, perceptual individuation load, and\ntransformation difficulty. Across all domains, our results revealed a\nconsistent pattern: decreased VLM accuracy was strongly correlated with\nincreased human reaction time (used as a proxy for serial processing load). As\ntasks require more demanding serial processing -- whether composing concepts,\nenumerating items, or performing mental transformations -- the VLM-human\nperformance gap widens reliably. These findings support our hypothesis,\nindicating that limitations in serial, visually grounded reasoning represent a\nfundamental bottleneck that distinguishes current VLMs from humans.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.25142v1",
    "published_at": "2025-09-29T17:51:20",
    "created_at": "2025-09-30T09:01:23.305227",
    "key_finding": "visual language models (vlms) show significant performance deficits in tasks requiring high serial processing demands, with increased difficulty correlating with a widening gap from human performance.",
    "methodology": "the study compared human and vlm performance across tasks in geometric reasoning, perceptual enumeration, and mental rotation, manipulating serial processing load through various factors.",
    "implications": "these results suggest that vlms face inherent limitations in visually grounded serial reasoning, highlighting a critical area for improvement in developing models that can better match human reasoning capabilities."
  },
  {
    "id": 606,
    "title": "ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory",
    "body": "With the growing adoption of large language model agents in persistent\nreal-world roles, they naturally encounter continuous streams of tasks. A key\nlimitation, however, is their failure to learn from the accumulated interaction\nhistory, forcing them to discard valuable insights and repeat past errors. We\npropose ReasoningBank, a novel memory framework that distills generalizable\nreasoning strategies from an agent's self-judged successful and failed\nexperiences. At test time, an agent retrieves relevant memories from\nReasoningBank to inform its interaction and then integrates new learnings back,\nenabling it to become more capable over time. Building on this powerful\nexperience learner, we further introduce memory-aware test-time scaling\n(MaTTS), which accelerates and diversifies this learning process by scaling up\nthe agent's interaction experience. By allocating more compute to each task,\nthe agent generates abundant, diverse experiences that provide rich contrastive\nsignals for synthesizing higher-quality memory. The better memory in turn\nguides more effective scaling, establishing a powerful synergy between memory\nand test-time scaling. Across web browsing and software engineering benchmarks,\nReasoningBank consistently outperforms existing memory mechanisms that store\nraw trajectories or only successful task routines, improving both effectiveness\nand efficiency; MaTTS further amplifies these gains. These findings establish\nmemory-driven experience scaling as a new scaling dimension, enabling agents to\nself-evolve with emergent behaviors naturally arise.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.25140v1",
    "published_at": "2025-09-29T17:51:03",
    "created_at": "2025-09-30T09:01:26.030197",
    "key_finding": "**** reasoningbank enhances agent performance by enabling self-evolution through a novel memory framework that generalizes reasoning strategies from both successful and failed experiences.",
    "methodology": "**** the framework incorporates memory-aware test-time scaling (matts), which increases computational resources for tasks, fostering diversified interactions and richer memory formation.",
    "implications": "**** this memory-driven approach improves both the effectiveness and efficiency of agents in tasks such as web browsing and software engineering, paving the way for agents to learn and adapt more effectively over time."
  },
  {
    "id": 597,
    "title": "VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing",
    "body": "The growing capabilities of large language models and multimodal systems have\nspurred interest in voice-first AI assistants, yet existing benchmarks are\ninadequate for evaluating the full range of these systems' capabilities. We\nintroduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI\nassistants across listening, speaking, and viewing. VoiceAssistant-Eval\ncomprises 10,497 curated examples spanning 13 task categories. These tasks\ninclude natural sounds, music, and spoken dialogue for listening; multi-turn\ndialogue, role-play imitation, and various scenarios for speaking; and highly\nheterogeneous images for viewing. To demonstrate its utility, we evaluate 21\nopen-source models and GPT-4o-Audio, measuring the quality of the response\ncontent and speech, as well as their consistency. The results reveal three key\nfindings: (1) proprietary models do not universally outperform open-source\nmodels; (2) most models excel at speaking tasks but lag in audio understanding;\nand (3) well-designed smaller models can rival much larger ones. Notably, the\nmid-sized Step-Audio-2-mini (7B) achieves more than double the listening\naccuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal\n(audio plus visual) input and role-play voice imitation tasks are difficult for\ncurrent models, and significant gaps persist in robustness and safety\nalignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous\nframework for evaluating and guiding the development of next-generation AI\nassistants. Code and data will be released at\nhttps://mathllm.github.io/VoiceAssistantEval/ .",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.22651v1",
    "published_at": "2025-09-26T17:59:59",
    "created_at": "2025-09-29T09:00:28.457853",
    "key_finding": "voiceassistant-eval reveals that proprietary ai assistants do not consistently outperform open-source models, with notable performance differences in tasks related to audio understanding and role-play imitation.",
    "methodology": "this benchmark evaluates 21 models across 13 task categories, utilizing 10,497 curated examples that assess listening, speaking, and viewing capabilities.",
    "implications": "the findings highlight existing gaps in multimodal input processing and safety alignment, providing a framework for future advancements in ai assistant development."
  },
  {
    "id": 598,
    "title": "See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation",
    "body": "We present See, Point, Fly (SPF), a training-free aerial vision-and-language\nnavigation (AVLN) framework built atop vision-language models (VLMs). SPF is\ncapable of navigating to any goal based on any type of free-form instructions\nin any kind of environment. In contrast to existing VLM-based approaches that\ntreat action prediction as a text generation task, our key insight is to\nconsider action prediction for AVLN as a 2D spatial grounding task. SPF\nharnesses VLMs to decompose vague language instructions into iterative\nannotation of 2D waypoints on the input image. Along with the predicted\ntraveling distance, SPF transforms predicted 2D waypoints into 3D displacement\nvectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the\ntraveling distance to facilitate more efficient navigation. Notably, SPF\nperforms navigation in a closed-loop control manner, enabling UAVs to follow\ndynamic targets in dynamic environments. SPF sets a new state of the art in DRL\nsimulation benchmark, outperforming the previous best method by an absolute\nmargin of 63%. In extensive real-world evaluations, SPF outperforms strong\nbaselines by a large margin. We also conduct comprehensive ablation studies to\nhighlight the effectiveness of our design choice. Lastly, SPF shows remarkable\ngeneralization to different VLMs. Project page: https://spf-web.pages.dev",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.22653v1",
    "published_at": "2025-09-26T17:59:59",
    "created_at": "2025-09-29T09:00:31.724523",
    "key_finding": "the see, point, fly (spf) framework sets a new state of the art in aerial vision-and-language navigation by outperforming previous methods in both simulation and real-world scenarios.",
    "methodology": "spf uniquely approaches action prediction as a 2d spatial grounding task, utilizing vision-language models to annotate waypoints on images and generate 3d displacement vectors for uav navigation, all without requiring prior training.",
    "implications": "this innovative framework enhances uav navigation efficiency and adaptability in dynamic environments, potentially transforming applications in areas such as search and rescue, surveillance, and delivery services."
  },
  {
    "id": 599,
    "title": "Toward a Physics of Deep Learning and Brains",
    "body": "Deep neural networks and brains both learn and share superficial\nsimilarities: processing nodes are likened to neurons and adjustable weights\nare likened to modifiable synapses. But can a unified theoretical framework be\nfound to underlie them both? Here we show that the equations used to describe\nneuronal avalanches in living brains can also be applied to cascades of\nactivity in deep neural networks. These equations are derived from\nnon-equilibrium statistical physics and show that deep neural networks learn\nbest when poised between absorbing and active phases. Because these networks\nare strongly driven by inputs, however, they do not operate at a true critical\npoint but within a quasi-critical regime -- one that still approximately\nsatisfies crackling noise scaling relations. By training networks with\ndifferent initializations, we show that maximal susceptibility is a more\nreliable predictor of learning than proximity to the critical point itself.\nThis provides a blueprint for engineering improved network performance.\nFinally, using finite-size scaling we identify distinct universality classes,\nincluding Barkhausen noise and directed percolation. This theoretical framework\ndemonstrates that universal features are shared by both biological and\nartificial neural networks.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.22649v1",
    "published_at": "2025-09-26T17:59:57",
    "created_at": "2025-09-29T09:00:34.375559",
    "key_finding": "the equations describing neuronal avalanches in biological brains can also explain activity cascades in deep neural networks, revealing a quasi-critical regime that enhances learning.",
    "methodology": "the research uses concepts from non-equilibrium statistical physics, analyzing training results of networks under various initializations to assess susceptibility and scaling relations.",
    "implications": "the findings suggest that by optimizing network configurations for maximal susceptibility rather than merely aiming for criticality, both biological and artificial neural networks can achieve improved performance, highlighting shared universal features."
  },
  {
    "id": 600,
    "title": "CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning",
    "body": "Image captioning is a fundamental task that bridges the visual and linguistic\ndomains, playing a critical role in pre-training Large Vision-Language Models\n(LVLMs). Current state-of-the-art captioning models are typically trained with\nSupervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable\ndata annotated by humans or proprietary models. This approach often leads to\nmodels that memorize specific ground-truth answers, limiting their generality\nand ability to generate diverse, creative descriptions. To overcome the\nlimitation of SFT, we propose applying the Reinforcement Learning with\nVerifiable Rewards (RLVR) paradigm to the open-ended task of image captioning.\nA primary challenge, however, is designing an objective reward function for the\ninherently subjective nature of what constitutes a \"good\" caption. We introduce\nCaptioning Reinforcement Learning (CapRL), a novel training framework that\nredefines caption quality through its utility: a high-quality caption should\nenable a non-visual language model to accurately answer questions about the\ncorresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM\ngenerates a caption, and the objective reward is derived from the accuracy of a\nseparate, vision-free LLM answering Multiple-Choice Questions based solely on\nthat caption. As the first study to apply RLVR to the subjective image\ncaptioning task, we demonstrate that CapRL significantly enhances multiple\nsettings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B\nresults in substantial gains across 12 benchmarks. Moreover, within the Prism\nFramework for caption quality evaluation, CapRL achieves performance comparable\nto Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%.\nCode is available here: https://github.com/InternLM/CapRL.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.22647v1",
    "published_at": "2025-09-26T17:59:55",
    "created_at": "2025-09-29T09:00:37.088944",
    "key_finding": "the caprl framework enhances image captioning capabilities by employing reinforcement learning with verifiable rewards, resulting in substantial performance improvements across multiple benchmarks.",
    "methodology": "caprl utilizes a two-stage pipeline where a large vision-language model generates captions, and reward signals for training are derived from a vision-free language model's accuracy in answering questions based on those captions.",
    "implications": "this novel approach overcomes limitations of traditional supervised fine-tuning by promoting diverse and contextually relevant captions, making it a significant advancement in the field of image captioning."
  },
  {
    "id": 601,
    "title": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs",
    "body": "Can humans identify AI-generated (fake) videos and provide grounded reasons?\nWhile video generation models have advanced rapidly, a critical dimension --\nwhether humans can detect deepfake traces within a generated video, i.e.,\nspatiotemporal grounded visual artifacts that reveal a video as machine\ngenerated -- has been largely overlooked. We introduce DeeptraceReward, the\nfirst fine-grained, spatially- and temporally- aware benchmark that annotates\nhuman-perceived fake traces for video generation reward. The dataset comprises\n4.3K detailed annotations across 3.3K high-quality generated videos. Each\nannotation provides a natural-language explanation, pinpoints a bounding-box\nregion containing the perceived trace, and marks precise onset and offset\ntimestamps. We consolidate these annotations into 9 major categories of\ndeepfake traces that lead humans to identify a video as AI-generated, and train\nmultimodal language models (LMs) as reward models to mimic human judgments and\nlocalizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by\n34.7% on average across fake clue identification, grounding, and explanation.\nInterestingly, we observe a consistent difficulty gradient: binary fake v.s.\nreal classification is substantially easier than fine-grained deepfake trace\ndetection; within the latter, performance degrades from natural language\nexplanations (easiest), to spatial grounding, to temporal labeling (hardest).\nBy foregrounding human-perceived deepfake traces, DeeptraceReward provides a\nrigorous testbed and training signal for socially aware and trustworthy video\ngeneration.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.22646v1",
    "published_at": "2025-09-26T17:59:54",
    "created_at": "2025-09-29T09:00:41.203262",
    "key_finding": "the study introduces deeptracereward, a benchmark for identifying human-perceived traces in ai-generated videos, demonstrating a significant improvement in detection performance over existing models.",
    "methodology": "the research annotates 4.3k deepfake traces in 3.3k videos, consolidating findings into 9 categories, and employs multimodal language models to mimic human judgments in identifying and explaining these traces.",
    "implications": "by highlighting human-perceived weaknesses in deepfake videos, deeptracereward serves as a crucial resource for developing more reliable and socially responsible video generation systems."
  },
  {
    "id": 592,
    "title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards",
    "body": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.21319v1",
    "published_at": "2025-09-25T16:19:06",
    "created_at": "2025-09-26T09:00:39.597031",
    "key_finding": "**** the proposed rlbff method bridges the strengths of reinforcement learning with human feedback (rlhf) and reinforcement learning with verifiable rewards (rlvr), allowing for the training of more nuanced reward models that achieve superior performance metrics.",
    "methodology": "",
    "implications": "**methodology:** rlbff utilizes binary evaluations from natural language feedback to encode principles into the reward model training as an entailment task, enhancing the understanding of response quality beyond simple correctness."
  },
  {
    "id": 593,
    "title": "SD3.5-Flash: Distribution-Guided Distillation of Generative Flows",
    "body": "We present SD3.5-Flash, an efficient few-step distillation framework that\nbrings high-quality image generation to accessible consumer devices. Our\napproach distills computationally prohibitive rectified flow models through a\nreformulated distribution matching objective tailored specifically for few-step\ngeneration. We introduce two key innovations: \"timestep sharing\" to reduce\ngradient noise and \"split-timestep fine-tuning\" to improve prompt alignment.\nCombined with comprehensive pipeline optimizations like text encoder\nrestructuring and specialized quantization, our system enables both rapid\ngeneration and memory-efficient deployment across different hardware\nconfigurations. This democratizes access across the full spectrum of devices,\nfrom mobile phones to desktop computers. Through extensive evaluation including\nlarge-scale user studies, we demonstrate that SD3.5-Flash consistently\noutperforms existing few-step methods, making advanced generative AI truly\naccessible for practical deployment.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.21318v1",
    "published_at": "2025-09-25T16:07:38",
    "created_at": "2025-09-26T09:00:41.997783",
    "key_finding": "sd3.5-flash significantly enhances the deployment of high-quality image generation on consumer devices by effectively distilling complex generative flow models.",
    "methodology": "the framework employs a distribution matching objective optimized for few-step generation, incorporating innovations like timestep sharing and split-timestep fine-tuning, alongside optimizations in the generation pipeline.",
    "implications": "this research promotes equitable access to advanced generative ai technologies across a wide range of devices, enabling practical applications in everyday scenarios."
  },
  {
    "id": 594,
    "title": "SAGE: A Realistic Benchmark for Semantic Understanding",
    "body": "As large language models (LLMs) achieve strong performance on traditional\nbenchmarks, there is an urgent need for more challenging evaluation frameworks\nthat probe deeper aspects of semantic understanding. We introduce SAGE\n(Semantic Alignment & Generalization Evaluation), a rigorous benchmark designed\nto assess both embedding models and similarity metrics across five categories:\nHuman Preference Alignment, Transformation Robustness, Information Sensitivity,\nClustering Performance, and Retrieval Robustness. Unlike existing benchmarks\nthat focus on isolated capabilities, SAGE evaluates semantic understanding\nthrough adversarial conditions, noisy transformations, and nuanced human\njudgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding\nmodels and classical metrics reveals significant performance gaps, with no\nsingle approach excelling across all dimensions. For instance, while\nstate-of-the-art embedding models like OpenAI's text-embedding-3-large dominate\nin aligning with human preferences (0.682 vs. 0.591 for the best classical\nmetric), they are significantly outperformed by classical metrics on\ninformation sensitivity tasks, where Jaccard Similarity achieves a score of\n0.905 compared to the top embedding score of 0.794. SAGE further uncovers\ncritical trade-offs: OpenAI's text-embedding-3-small achieves the highest\nclustering performance (0.483) but demonstrates extreme brittleness with the\nlowest robustness score (0.011). SAGE exposes critical limitations in current\nsemantic understanding capabilities and provides a more realistic assessment of\nmodel robustness for real-world deployment.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.21310v1",
    "published_at": "2025-09-25T15:27:15",
    "created_at": "2025-09-26T09:00:45.299799",
    "key_finding": "the sage benchmark reveals significant performance gaps among various embedding models and metrics in semantic understanding tasks, indicating no single approach excels across all dimensions.",
    "methodology": "sage evaluates embedding models using 30+ datasets across five categories, including adversarial conditions and human judgment tasks, to provide a comprehensive assessment of semantic understanding.",
    "implications": "the findings highlight critical limitations in current models and suggest the need for robust evaluations in real-world applications, urging further development in semantic understanding technologies."
  },
  {
    "id": 595,
    "title": "No Prior, No Leakage: Revisiting Reconstruction Attacks in Trained Neural Networks",
    "body": "The memorization of training data by neural networks raises pressing concerns\nfor privacy and security. Recent work has shown that, under certain conditions,\nportions of the training set can be reconstructed directly from model\nparameters. Some of these methods exploit implicit bias toward margin\nmaximization, suggesting that properties often regarded as beneficial for\ngeneralization may actually compromise privacy. Yet despite striking empirical\ndemonstrations, the reliability of these attacks remains poorly understood and\nlacks a solid theoretical foundation. In this work, we take a complementary\nperspective: rather than designing stronger attacks, we analyze the inherent\nweaknesses and limitations of existing reconstruction methods and identify\nconditions under which they fail. We rigorously prove that, without\nincorporating prior knowledge about the data, there exist infinitely many\nalternative solutions that may lie arbitrarily far from the true training set,\nrendering reconstruction fundamentally unreliable. Empirically, we further\ndemonstrate that exact duplication of training examples occurs only by chance.\nOur results refine the theoretical understanding of when training set leakage\nis possible and offer new insights into mitigating reconstruction attacks.\nRemarkably, we demonstrate that networks trained more extensively, and\ntherefore satisfying implicit bias conditions more strongly -- are, in fact,\nless susceptible to reconstruction attacks, reconciling privacy with the need\nfor strong generalization in this setting.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.21296v1",
    "published_at": "2025-09-25T15:14:08",
    "created_at": "2025-09-26T09:00:47.988601",
    "key_finding": "this research proves that reconstruction attacks on neural networks are fundamentally unreliable without prior knowledge about the training data, as there are infinitely many alternative solutions that can diverge from the true data.",
    "methodology": "the study includes rigorous theoretical analysis and empirical validation to identify the limitations of existing reconstruction methods and their conditions of failure.",
    "implications": "the findings suggest that greater training—resulting in stronger implicit bias—can actually enhance privacy while maintaining strong generalization, providing new insights into designing safer neural networks."
  },
  {
    "id": 596,
    "title": "Optimal Robust Recourse with $L^p$-Bounded Model Change",
    "body": "Recourse provides individuals who received undesirable labels (e.g., denied a\nloan) from algorithmic decision-making systems with a minimum-cost improvement\nsuggestion to achieve the desired outcome. However, in practice, models often\nget updated to reflect changes in the data distribution or environment,\ninvalidating the recourse recommendations (i.e., following the recourse will\nnot lead to the desirable outcome). The robust recourse literature addresses\nthis issue by providing a framework for computing recourses whose validity is\nresilient to slight changes in the model. However, since the optimization\nproblem of computing robust recourse is non-convex (even for linear models),\nmost of the current approaches do not have any theoretical guarantee on the\noptimality of the recourse. Recent work by Kayastha et. al. provides the first\nprovably optimal algorithm for robust recourse with respect to generalized\nlinear models when the model changes are measured using the $L^{\\infty}$ norm.\nHowever, using the $L^{\\infty}$ norm can lead to recourse solutions with a high\nprice. To address this shortcoming, we consider more constrained model changes\ndefined by the $L^p$ norm, where $p\\geq 1$ but $p\\neq \\infty$, and provide a\nnew algorithm that provably computes the optimal robust recourse for\ngeneralized linear models. Empirically, for both linear and non-linear models,\nwe demonstrate that our algorithm achieves a significantly lower price of\nrecourse (up to several orders of magnitude) compared to prior work and also\nexhibits a better trade-off between the implementation cost of recourse and its\nvalidity. Our empirical analysis also illustrates that our approach provides\nmore sparse recourses compared to prior work and remains resilient to\npost-processing approaches that guarantee feasibility.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.21293v1",
    "published_at": "2025-09-25T15:11:51",
    "created_at": "2025-09-26T09:00:50.996164",
    "key_finding": "the proposed algorithm for robust recourse using $l^p$-bounded model changes significantly lowers the cost of achieving desirable outcomes compared to existing methods.",
    "methodology": "the research develops a new algorithm that optimally computes robust recourse for generalized linear models under $l^p$ constraints (where $1 \\leq p < \\infty$), addressing the limitations of the $l^\\infty$ norm approach.",
    "implications": "this work enhances the feasibility and cost-effectiveness of recourse recommendations in algorithmic decision-making, promoting more resilient and sparse solutions that maintain their validity despite model updates."
  },
  {
    "id": 587,
    "title": "EmbeddingGemma: Powerful and Lightweight Text Representations",
    "body": "We introduce EmbeddingGemma, a new lightweight, open text embedding model\nbased on the Gemma 3 language model family. Our innovative training recipe\nstrategically captures knowledge from larger models via encoder-decoder\ninitialization and geometric embedding distillation. We improve model\nrobustness and expressiveness with a spread-out regularizer, and ensure\ngeneralizability by merging checkpoints from varied, optimized mixtures.\nEvaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual,\nEnglish, and code domains, EmbeddingGemma (300M) achieves state-of-the-art\nresults. Notably, it outperforms prior top models, both proprietary and open,\nwith fewer than 500M parameters, and provides performance comparable to models\ndouble its size, offering an exceptional performance-to-cost ratio. Remarkably,\nthis lead persists when quantizing model weights or truncating embedding\noutputs. This makes EmbeddingGemma particularly well-suited for low-latency and\nhigh-throughput use cases such as on-device applications. We provide ablation\nstudies exploring our key design choices. We release EmbeddingGemma to the\ncommunity to promote further research.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.20354v1",
    "published_at": "2025-09-24T17:56:51",
    "created_at": "2025-09-25T09:00:34.616472",
    "key_finding": "embeddinggemma is a lightweight text embedding model achieving state-of-the-art performance on the massive text embedding benchmark while having fewer than 500m parameters.",
    "methodology": "the model utilizes a novel training approach involving encoder-decoder initialization, geometric embedding distillation, and a spread-out regularizer, combined with checkpoints from diverse mixtures to enhance robustness and generalizability.",
    "implications": "its efficient performance-to-cost ratio and suitability for low-latency applications make embeddinggemma a valuable tool for on-device implementations in multilingual, english, and code domains, encouraging further research in text representation models."
  },
  {
    "id": 588,
    "title": "Process-Informed Forecasting of Complex Thermal Dynamics in Pharmaceutical Manufacturing",
    "body": "Accurate time-series forecasting for complex physical systems is the backbone\nof modern industrial monitoring and control. While deep learning models excel\nat capturing complex dynamics, currently, their deployment is limited due to\nphysical inconsistency and robustness, hence constraining their reliability in\nregulated environments. We introduce process-informed forecasting (PIF) models\nfor temperature in pharmaceutical lyophilization. We investigate a wide range\nof models, from classical ones such as Autoregressive Integrated Moving Average\nModel (ARIMA) and Exponential Smoothing Model (ETS), to modern deep learning\narchitectures, including Kolmogorov-Arnold Networks (KANs). We compare three\ndifferent loss function formulations that integrate a process-informed\ntrajectory prior: a fixed-weight loss, a dynamic uncertainty-based loss, and a\nResidual-Based Attention (RBA) mechanism. We evaluate all models not only for\naccuracy and physical consistency but also for robustness to sensor noise.\nFurthermore, we test the practical generalizability of the best model in a\ntransfer learning scenario on a new process. Our results show that PIF models\noutperform their data-driven counterparts in terms of accuracy, physical\nplausibility and noise resilience. This work provides a roadmap for developing\nreliable and generalizable forecasting solutions for critical applications in\nthe pharmaceutical manufacturing landscape.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.20349v1",
    "published_at": "2025-09-24T17:42:00",
    "created_at": "2025-09-25T09:00:37.609847",
    "key_finding": "process-informed forecasting (pif) models significantly enhance the accuracy, physical plausibility, and noise resilience of temperature predictions in pharmaceutical lyophilization compared to traditional data-driven models.",
    "methodology": "the research systematically evaluates a range of forecasting models, including classical approaches (arima, ets) and deep learning architectures (kans), incorporating innovative loss functions that utilize process-informed trajectory priors.",
    "implications": "this study offers a critical framework for developing robust and generalizable forecasting solutions, addressing the reliability issues of deep learning in regulated environments and enhancing industrial monitoring in pharmaceutical manufacturing."
  },
  {
    "id": 589,
    "title": "Statistical Inference Leveraging Synthetic Data with Distribution-Free Guarantees",
    "body": "The rapid proliferation of high-quality synthetic data -- generated by\nadvanced AI models or collected as auxiliary data from related tasks --\npresents both opportunities and challenges for statistical inference. This\npaper introduces a GEneral Synthetic-Powered Inference (GESPI) framework that\nwraps around any statistical inference procedure to safely enhance sample\nefficiency by combining synthetic and real data. Our framework leverages\nhigh-quality synthetic data to boost statistical power, yet adaptively defaults\nto the standard inference method using only real data when synthetic data is of\nlow quality. The error of our method remains below a user-specified bound\nwithout any distributional assumptions on the synthetic data, and decreases as\nthe quality of the synthetic data improves. This flexibility enables seamless\nintegration with conformal prediction, risk control, hypothesis testing, and\nmultiple testing procedures, all without modifying the base inference method.\nWe demonstrate the benefits of our method on challenging tasks with limited\nlabeled data, including AlphaFold protein structure prediction, and comparing\nlarge reasoning models on complex math problems.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.20345v1",
    "published_at": "2025-09-24T17:37:14",
    "created_at": "2025-09-25T09:00:40.276763",
    "key_finding": "the gespi framework effectively enhances statistical inference by integrating synthetic data while maintaining error bounds without distributional assumptions.",
    "methodology": "the framework adapts inference methods to use both high-quality synthetic data and real data, defaulting to real data when synthetic data quality is low, ensuring robust performance across various statistical procedures.",
    "implications": "this approach enables improved sample efficiency and adaptability in statistical analysis, particularly beneficial for tasks with limited labeled data, such as protein structure prediction and complex reasoning challenges."
  },
  {
    "id": 590,
    "title": "Morphological Synthesizer for Ge'ez Language: Addressing Morphological Complexity and Resource Limitations",
    "body": "Ge'ez is an ancient Semitic language renowned for its unique alphabet. It\nserves as the script for numerous languages, including Tigrinya and Amharic,\nand played a pivotal role in Ethiopia's cultural and religious development\nduring the Aksumite kingdom era. Ge'ez remains significant as a liturgical\nlanguage in Ethiopia and Eritrea, with much of the national identity\ndocumentation recorded in Ge'ez. These written materials are invaluable primary\nsources for studying Ethiopian and Eritrean philosophy, creativity, knowledge,\nand civilization. Ge'ez has a complex morphological structure with rich\ninflectional and derivational morphology, and no usable NLP has been developed\nand published until now due to the scarcity of annotated linguistic data,\ncorpora, labeled datasets, and lexicons. Therefore, we propose a rule-based\nGe'ez morphological synthesizer to generate surface words from root words\naccording to the morphological structures of the language. We used 1,102 sample\nverbs, representing all verb morphological structures, to test and evaluate the\nsystem. The system achieves a performance of 97.4%, outperforming the baseline\nmodel and suggesting that future work should build a comprehensive system\nconsidering morphological variations of the language.\n  Keywords: Ge'ez, NLP, morphology, morphological synthesizer, rule-based",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.20341v1",
    "published_at": "2025-09-24T17:33:47",
    "created_at": "2025-09-25T09:00:42.962142",
    "key_finding": "the study presents a rule-based morphological synthesizer for the ge'ez language, achieving a 97.4% performance in generating surface words from root words based on its complex morphological structures.",
    "methodology": "the synthesizer was evaluated using a dataset of 1,102 sample verbs that encompass all verb morphological structures of ge'ez.",
    "implications": "this research addresses the significant resource limitations in ge'ez nlp and lays the groundwork for future development of comprehensive systems that account for the language's morphological variations."
  },
  {
    "id": 591,
    "title": "Spatio-Temporal Directed Graph Learning for Account Takeover Fraud Detection",
    "body": "Account Takeover (ATO) fraud poses a significant challenge in consumer\nbanking, requiring high recall under strict latency while minimizing friction\nfor legitimate users. Production systems typically rely on tabular\ngradient-boosted decision trees (e.g., XGBoost) that score sessions\nindependently, overlooking the relational and temporal structure of online\nactivity that characterizes coordinated attacks and \"fraud rings.\" We introduce\nATLAS (Account Takeover Learning Across Spatio-Temporal Directed Graph), a\nframework that reformulates ATO detection as spatio-temporal node\nclassification on a time-respecting directed session graph. ATLAS links\nentities via shared identifiers (account, device, IP) and regulates\nconnectivity with time-window and recency constraints, enabling causal,\ntime-respecting message passing and latency-aware label propagation that uses\nonly labels available at scoring time, non-anticipative and leakage-free. We\noperationalize ATLAS with inductive GraphSAGE variants trained via neighbor\nsampling, at scale on a sessions graph with more than 100M nodes and around 1B\nedges. On a high-risk digital product at Capital One, ATLAS delivers 6.38\npercent AUC improvement and more than 50 percent reduction in customer\nfriction, improving fraud capture while reducing user friction.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.20339v1",
    "published_at": "2025-09-24T17:32:24",
    "created_at": "2025-09-25T09:00:46.589022",
    "key_finding": "the atlas framework significantly enhances account takeover (ato) fraud detection by achieving a 6.38% improvement in auc while reducing customer friction by over 50%.",
    "methodology": "atlas reformulates ato detection as spatio-temporal node classification on a directed session graph, utilizing causal message passing and inductive graphsage variants trained on a large-scale graph.",
    "implications": "this approach leverages the relational and temporal structure of online activities, offering a more effective detection mechanism for coordinated fraud attacks while maintaining a seamless user experience."
  },
  {
    "id": 581,
    "title": "Residual Off-Policy RL for Finetuning Behavior Cloning Policies",
    "body": "Recent advances in behavior cloning (BC) have enabled impressive visuomotor\ncontrol policies. However, these approaches are limited by the quality of human\ndemonstrations, the manual effort required for data collection, and the\ndiminishing returns from increasing offline data. In comparison, reinforcement\nlearning (RL) trains an agent through autonomous interaction with the\nenvironment and has shown remarkable success in various domains. Still,\ntraining RL policies directly on real-world robots remains challenging due to\nsample inefficiency, safety concerns, and the difficulty of learning from\nsparse rewards for long-horizon tasks, especially for high-degree-of-freedom\n(DoF) systems. We present a recipe that combines the benefits of BC and RL\nthrough a residual learning framework. Our approach leverages BC policies as\nblack-box bases and learns lightweight per-step residual corrections via\nsample-efficient off-policy RL. We demonstrate that our method requires only\nsparse binary reward signals and can effectively improve manipulation policies\non high-degree-of-freedom (DoF) systems in both simulation and the real world.\nIn particular, we demonstrate, to the best of our knowledge, the first\nsuccessful real-world RL training on a humanoid robot with dexterous hands. Our\nresults demonstrate state-of-the-art performance in various vision-based tasks,\npointing towards a practical pathway for deploying RL in the real world.\nProject website: https://residual-offpolicy-rl.github.io",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.19301v1",
    "published_at": "2025-09-23T17:59:46",
    "created_at": "2025-09-24T09:00:43.139285",
    "key_finding": "the proposed residual off-policy reinforcement learning (rl) approach significantly enhances behavior cloning (bc) policies by learning lightweight per-step corrections, achieving state-of-the-art performance on high-degree-of-freedom robotic manipulation tasks.",
    "methodology": "this framework combines bc as a black-box foundation with off-policy rl to refine policies using sparse binary rewards, enabling effective training in both simulation and real-world settings.",
    "implications": "the successful application of this method on a humanoid robot with dexterous hands indicates a promising direction for practical rl deployments in complex, real-world environments."
  },
  {
    "id": 582,
    "title": "Audio-Based Pedestrian Detection in the Presence of Vehicular Noise",
    "body": "Audio-based pedestrian detection is a challenging task and has, thus far,\nonly been explored in noise-limited environments. We present a new dataset,\nresults, and a detailed analysis of the state-of-the-art in audio-based\npedestrian detection in the presence of vehicular noise. In our study, we\nconduct three analyses: (i) cross-dataset evaluation between noisy and\nnoise-limited environments, (ii) an assessment of the impact of noisy data on\nmodel performance, highlighting the influence of acoustic context, and (iii) an\nevaluation of the model's predictive robustness on out-of-domain sounds. The\nnew dataset is a comprehensive 1321-hour roadside dataset. It incorporates\ntraffic-rich soundscapes. Each recording includes 16kHz audio synchronized with\nframe-level pedestrian annotations and 1fps video thumbnails.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.19295v1",
    "published_at": "2025-09-23T17:57:44",
    "created_at": "2025-09-24T09:00:47.172994",
    "key_finding": "the study demonstrates that audio-based pedestrian detection can be effectively trained and evaluated even in the presence of vehicular noise, challenging previous assumptions of noise-limited detection.",
    "methodology": "the researchers created a robust 1321-hour dataset featuring diverse traffic soundscapes, conducted cross-dataset evaluations, assessed model performance under noise conditions, and tested predictive robustness on out-of-domain sounds.",
    "implications": "this research highlights the potential for improving pedestrian detection systems in real-world settings, suggesting that audio data can be leveraged to enhance safety in environments typically dominated by vehicular noise."
  },
  {
    "id": 583,
    "title": "SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration",
    "body": "Intelligent agents progress by continually refining their capabilities\nthrough actively exploring environments. Yet robot policies often lack\nsufficient exploration capability due to action mode collapse. Existing methods\nthat encourage exploration typically rely on random perturbations, which are\nunsafe and induce unstable, erratic behaviors, thereby limiting their\neffectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a\nframework that enhances policy exploration and improvement in robotic\nmanipulation. SOE learns a compact latent representation of task-relevant\nfactors and constrains exploration to the manifold of valid actions, ensuring\nsafety, diversity, and effectiveness. It can be seamlessly integrated with\narbitrary policy models as a plug-in module, augmenting exploration without\ndegrading the base policy performance. Moreover, the structured latent space\nenables human-guided exploration, further improving efficiency and\ncontrollability. Extensive experiments in both simulation and real-world tasks\ndemonstrate that SOE consistently outperforms prior methods, achieving higher\ntask success rates, smoother and safer exploration, and superior sample\nefficiency. These results establish on-manifold exploration as a principled\napproach to sample-efficient policy self-improvement. Project website:\nhttps://ericjin2002.github.io/SOE",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.19292v1",
    "published_at": "2025-09-23T17:54:47",
    "created_at": "2025-09-24T09:00:50.090094",
    "key_finding": "the soe framework significantly enhances robotic policy exploration and improvement by ensuring safe and effective on-manifold exploration, leading to higher task success rates and superior sample efficiency compared to existing methods.",
    "methodology": "soe learns a compact latent representation of task-relevant factors and constrains exploration to the manifold of valid actions, allowing it to be integrated into various policy models as a plug-in module.",
    "implications": "by establishing on-manifold exploration as a principled approach, soe not only improves robot performance but also enables safer and more controllable exploration, making it a valuable tool for advancing intelligent agent capabilities in diverse environments."
  },
  {
    "id": 584,
    "title": "What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT",
    "body": "Large reasoning models (LRMs) spend substantial test-time compute on long\nchain-of-thought (CoT) traces, but what *characterizes* an effective CoT\nremains unclear. While prior work reports gains from lengthening CoTs and\nincreasing review (revisiting earlier steps) via appended *wait* tokens, recent\nstudies suggest that shorter thinking can outperform longer traces. We\ntherefore conduct a systematic evaluation across ten LRMs on math and\nscientific reasoning. Contrary to the \"longer-is-better\" narrative, we find\nthat both naive CoT lengthening and increased review are associated with\n*lower* accuracy.\n  As CoT unfolds step by step, token-level metrics can conflate verbosity with\nprocess quality. We introduce a graph view of CoT to extract structure and\nidentify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of\nsteps in abandoned branches-that consistently outpredicts length and review\nratio for correctness across models. To probe causality, we design two\ninterventions. First, we rank candidate CoTs by each metric at test time, where\nFSF yields the largest pass@1 gains; second, we edit CoTs to remove failed\nbranches, which significantly improves accuracy, indicating that failed\nbranches bias subsequent reasoning. Taken together, these results characterize\neffective CoTs as those that *fail less* and support *structure-aware*\ntest-time scaling over indiscriminately generating long CoT.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.19284v1",
    "published_at": "2025-09-23T17:50:54",
    "created_at": "2025-09-24T09:00:53.772907",
    "key_finding": "effective chain-of-thought (cot) reasoning is characterized by a lower failed-step fraction (fsf), which correlates with higher accuracy, contradicting the notion that longer cots are inherently better.",
    "methodology": "the study systematically evaluates ten large reasoning models (lrms) on math and scientific reasoning, introducing the fsf metric and conducting interventions to improve cot quality by ranking and editing cots based on failed steps.",
    "implications": "this research suggests that focusing on minimizing failure in reasoning steps, rather than merely increasing the length or review of cots, can enhance the performance of lrms, promoting a structure-aware approach to cot generation."
  },
  {
    "id": 585,
    "title": "segment anything model 2 for mri neurobroma segmentation",
    "body": "Background and Objectives: Neurofibromatosis type 1 is a genetic disorder\ncharacterized by the development of numerous neurofibromas (NFs) throughout the\nbody. Whole-body MRI (WB-MRI) is the clinical standard for detection and\nlongitudinal surveillance of NF tumor growth. Existing interactive segmentation\nmethods fail to combine high lesion-wise precision with scalability to hundreds\nof lesions. This study proposes a novel interactive segmentation model tailored\nto this challenge.\n  Methods: We introduce MOIS-SAM2, a multi-object interactive segmentation\nmodel that extends the state-of-the-art, transformer-based, promptable Segment\nAnything Model 2 (SAM2) with exemplar-based semantic propagation. MOIS-SAM2 was\ntrained and evaluated on 119 WB-MRI scans from 84 NF1 patients acquired using\nT2-weighted fat-suppressed sequences. The dataset was split at the patient\nlevel into a training set and four test sets (one in-domain and three\nreflecting different domain shift scenarios, e.g., MRI field strength\nvariation, low tumor burden, differences in clinical site and scanner vendor).\n  Results: On the in-domain test set, MOIS-SAM2 achieved a scan-wise DSC of\n0.60 against expert manual annotations, outperforming baseline 3D nnU-Net (DSC:\n0.54) and SAM2 (DSC: 0.35). Performance of the proposed model was maintained\nunder MRI field strength shift (DSC: 0.53) and scanner vendor variation (DSC:\n0.50), and improved in low tumor burden cases (DSC: 0.61). Lesion detection F1\nscores ranged from 0.62 to 0.78 across test sets. Preliminary inter-reader\nvariability analysis showed model-to-expert agreement (DSC: 0.62-0.68),\ncomparable to inter-expert agreement (DSC: 0.57-0.69).\n  Conclusions: The proposed MOIS-SAM2 enables efficient and scalable\ninteractive segmentation of NFs in WB-MRI with minimal user input and strong\ngeneralization, supporting integration into clinical workflows.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.19277v1",
    "published_at": "2025-09-23T17:42:24",
    "created_at": "2025-09-24T09:01:00.318927",
    "key_finding": "mois-sam2 achieved superior interactive segmentation of neurofibromas in whole-body mri with a dsc of 0.60, outperforming existing models.",
    "methodology": "the model employs exemplar-based semantic propagation within an advanced transformer-based segment anything model 2 framework, trained on 119 wb-mri scans from nf1 patients.",
    "implications": "mois-sam2 demonstrates strong generalization across various clinical contexts, enabling efficient lesion segmentation that could enhance clinical workflow for neurofibromatosis surveillance."
  },
  {
    "id": 586,
    "title": "exemplar-based sam2 for neurofibroma segmentation in mri",
    "body": "Background and Objectives: Neurofibromatosis type 1 is a genetic disorder\ncharacterized by the development of numerous neurofibromas (NFs) throughout the\nbody. Whole-body MRI (WB-MRI) is the clinical standard for detection and\nlongitudinal surveillance of NF tumor growth. Existing interactive segmentation\nmethods fail to combine high lesion-wise precision with scalability to hundreds\nof lesions. This study proposes a novel interactive segmentation model tailored\nto this challenge.\n  Methods: We introduce MOIS-SAM2, a multi-object interactive segmentation\nmodel that extends the state-of-the-art, transformer-based, promptable Segment\nAnything Model 2 (SAM2) with exemplar-based semantic propagation. MOIS-SAM2 was\ntrained and evaluated on 119 WB-MRI scans from 84 NF1 patients acquired using\nT2-weighted fat-suppressed sequences. The dataset was split at the patient\nlevel into a training set and four test sets (one in-domain and three\nreflecting different domain shift scenarios, e.g., MRI field strength\nvariation, low tumor burden, differences in clinical site and scanner vendor).\n  Results: On the in-domain test set, MOIS-SAM2 achieved a scan-wise DSC of\n0.60 against expert manual annotations, outperforming baseline 3D nnU-Net (DSC:\n0.54) and SAM2 (DSC: 0.35). Performance of the proposed model was maintained\nunder MRI field strength shift (DSC: 0.53) and scanner vendor variation (DSC:\n0.50), and improved in low tumor burden cases (DSC: 0.61). Lesion detection F1\nscores ranged from 0.62 to 0.78 across test sets. Preliminary inter-reader\nvariability analysis showed model-to-expert agreement (DSC: 0.62-0.68),\ncomparable to inter-expert agreement (DSC: 0.57-0.69).\n  Conclusions: The proposed MOIS-SAM2 enables efficient and scalable\ninteractive segmentation of NFs in WB-MRI with minimal user input and strong\ngeneralization, supporting integration into clinical workflows.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.19277v2",
    "published_at": "2025-09-23T17:42:24",
    "created_at": "2025-09-25T02:00:53.628361",
    "key_finding": "the mois-sam2 model achieves superior interactive segmentation of neurofibromas in whole-body mri, with an in-domain dsc of 0.60, outperforming existing methods.",
    "methodology": "mois-sam2 enhances the promptable segment anything model 2 with exemplar-based semantic propagation, and was trained on 119 wb-mri scans from nf1 patients, evaluated across various domain shift scenarios.",
    "implications": "this model demonstrates strong generalization and efficiency in segmenting numerous neurofibromas, making it a viable tool for clinical integration in nf surveillance."
  },
  {
    "id": 576,
    "title": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning",
    "body": "Recent advances in Large Multi-modal Models (LMMs) have demonstrated their\nremarkable success as general-purpose multi-modal assistants, with particular\nfocuses on holistic image- and video-language understanding. Conversely, less\nattention has been given to scaling fine-grained pixel-level understanding\ncapabilities, where the models are expected to realize pixel-level alignment\nbetween visual signals and language semantics. Some previous studies have\napplied LMMs to related tasks such as region-level captioning and referring\nexpression segmentation. However, these models are limited to performing either\nreferring or segmentation tasks independently and fail to integrate these\nfine-grained perception capabilities into visual reasoning. To bridge this gap,\nwe propose UniPixel, a large multi-modal model capable of flexibly\ncomprehending visual prompt inputs and generating mask-grounded responses. Our\nmodel distinguishes itself by seamlessly integrating pixel-level perception\nwith general visual understanding capabilities. Specifically, UniPixel\nprocesses visual prompts and generates relevant masks on demand, and performs\nsubsequent reasoning conditioning on these intermediate pointers during\ninference, thereby enabling fine-grained pixel-level reasoning. The\neffectiveness of our approach has been verified on 10 benchmarks across a\ndiverse set of tasks, including pixel-level referring/segmentation and\nobject-centric understanding in images/videos. A novel PixelQA task that\njointly requires referring, segmentation, and question answering is also\ndesigned to verify the flexibility of our method.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.18094v1",
    "published_at": "2025-09-22T17:59:40",
    "created_at": "2025-09-23T09:00:36.110466",
    "key_finding": "the unipixel model successfully integrates pixel-level referring and segmentation capabilities, enhancing visual reasoning across various tasks.",
    "methodology": "it operates by processing visual prompts to generate masks and performs reasoning based on these masks during inference, validated on 10 diverse benchmarks including a newly designed pixelqa task.",
    "implications": "this work advances the field of fine-grained pixel-level understanding in multi-modal models, potentially improving applications in image and video analysis that require detailed semantic comprehension."
  },
  {
    "id": 577,
    "title": "SEQR: Secure and Efficient QR-based LoRA Routing",
    "body": "Low-Rank Adaptation (LoRA) has become a standard technique for\nparameter-efficient fine-tuning of large language models, enabling large\nlibraries of LoRAs, each for a specific task or domain. Efficiently selecting\nthe correct LoRA adapter for a given input remains a challenge, particularly in\nsecure environments where supervised training of routers may raise privacy\nconcerns. Motivated by previous approaches, we formalize the goal of\nunsupervised LoRA routing in terms of activation norm maximization, providing a\ntheoretical framework for analysis. We demonstrate the discriminative power of\nactivation norms and introduce SEQR, an unsupervised LoRA routing algorithm\ndesigned to maximize efficiency while providing strict routing guarantees. SEQR\nprovably identifies the norm-maximizing adapter with significantly greater\nefficiency, making it a highly scalable and effective solution for dynamic LoRA\ncomposition. We validate our results through experiments that demonstrate\nimproved multi-task performance and efficiency.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.18093v1",
    "published_at": "2025-09-22T17:59:38",
    "created_at": "2025-09-23T09:00:41.110462",
    "key_finding": "**** seqr is an unsupervised routing algorithm that efficiently identifies the optimal low-rank adaptation (lora) adapter for given inputs, maximizing activation norms in secure environments.",
    "methodology": "**** the paper formalizes unsupervised lora routing through activation norm maximization and uses a theoretical framework to demonstrate the discriminative power of activation norms for algorithm development.",
    "implications": "**** seqr enhances the scalability and performance of multi-task learning with lora modules, ensuring robust routing performance while addressing privacy concerns typically associated with supervised training methods."
  },
  {
    "id": 578,
    "title": "OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System",
    "body": "Despite the growing interest in replicating the scaled success of large\nlanguage models (LLMs) in industrial search and recommender systems, most\nexisting industrial efforts remain limited to transplanting Transformer\narchitectures, which bring only incremental improvements over strong Deep\nLearning Recommendation Models (DLRMs). From a first principle perspective, the\nbreakthroughs of LLMs stem not only from their architectures but also from two\ncomplementary mechanisms: context engineering, which enriches raw input queries\nwith contextual cues to better elicit model capabilities, and multi-step\nreasoning, which iteratively refines model outputs through intermediate\nreasoning paths. However, these two mechanisms and their potential to unlock\nsubstantial improvements remain largely underexplored in industrial ranking\nsystems.\n  In this paper, we propose OnePiece, a unified framework that seamlessly\nintegrates LLM-style context engineering and reasoning into both retrieval and\nranking models of industrial cascaded pipelines. OnePiece is built on a pure\nTransformer backbone and further introduces three key innovations: (1)\nstructured context engineering, which augments interaction history with\npreference and scenario signals and unifies them into a structured tokenized\ninput sequence for both retrieval and ranking; (2) block-wise latent reasoning,\nwhich equips the model with multi-step refinement of representations and scales\nreasoning bandwidth via block size; (3) progressive multi-task training, which\nleverages user feedback chains to effectively supervise reasoning steps during\ntraining. OnePiece has been deployed in the main personalized search scenario\nof Shopee and achieves consistent online gains across different key business\nmetrics, including over $+2\\%$ GMV/UU and a $+2.90\\%$ increase in advertising\nrevenue.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.18091v1",
    "published_at": "2025-09-22T17:59:07",
    "created_at": "2025-09-23T09:00:44.994776",
    "key_finding": "**** onepiece significantly enhances industrial cascade ranking systems by integrating llm-inspired context engineering and multi-step reasoning, resulting in measurable improvements in business metrics.",
    "methodology": "**** the framework employs a pure transformer backbone, introducing structured context engineering, block-wise latent reasoning, and progressive multi-task training to enrich input data and refine outputs within retrieval and ranking models.",
    "implications": "**** by deploying onepiece in shopee's personalized search, the approach demonstrated consistent online gains in key performance indicators, highlighting the value of advanced context and reasoning techniques in commercial applications."
  },
  {
    "id": 579,
    "title": "evolving coordination in multi-agent systems",
    "body": "Decentralized combinatorial optimization in evolving multi-agent systems\nposes significant challenges, requiring agents to balance long-term\ndecision-making, short-term optimized collective outcomes, while preserving\nautonomy of interactive agents under unanticipated changes. Reinforcement\nlearning offers a way to model sequential decision-making through dynamic\nprogramming to anticipate future environmental changes. However, applying\nmulti-agent reinforcement learning (MARL) to decentralized combinatorial\noptimization problems remains an open challenge due to the exponential growth\nof the joint state-action space, high communication overhead, and privacy\nconcerns in centralized training. To address these limitations, this paper\nproposes Hierarchical Reinforcement and Collective Learning (HRCL), a novel\napproach that leverages both MARL and decentralized collective learning based\non a hierarchical framework. Agents take high-level strategies using MARL to\ngroup possible plans for action space reduction and constrain the agent\nbehavior for Pareto optimality. Meanwhile, the low-level collective learning\nlayer ensures efficient and decentralized coordinated decisions among agents\nwith minimal communication. Extensive experiments in a synthetic scenario and\nreal-world smart city application models, including energy self-management and\ndrone swarm sensing, demonstrate that HRCL significantly improves performance,\nscalability, and adaptability compared to the standalone MARL and collective\nlearning approaches, achieving a win-win synthesis solution.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.18088v1",
    "published_at": "2025-09-22T17:58:45",
    "created_at": "2025-09-23T09:00:51.459549",
    "key_finding": "**** the hierarchical reinforcement and collective learning (hrcl) approach enhances performance and scalability in decentralized combinatorial optimization tasks for evolving multi-agent systems.",
    "methodology": "**** hrcl combines multi-agent reinforcement learning (marl) with a decentralized collective learning framework, utilizing high-level marl strategies for action space reduction and low-level coordinated decisions with minimal communication.",
    "implications": "**** this research paves the way for more efficient and adaptable multi-agent systems in practical applications, such as smart city management and drone coordination, by addressing challenges related to joint state-action space complexity and communication overhead."
  },
  {
    "id": 580,
    "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding",
    "body": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.18085v1",
    "published_at": "2025-09-22T17:58:21",
    "created_at": "2025-09-23T09:00:54.661782",
    "key_finding": "the spiffy algorithm accelerates diffusion llm inference by 2.8-3.1 times while maintaining the integrity of the model's output distribution.",
    "methodology": "spiffy introduces draft states through an auto-speculative approach using a directed draft graph designed for the bidirectional nature of dllm generation, paired with an offline calibration algorithm for optimal graph configurations.",
    "implications": "this method significantly enhances dllm generation speeds, enabling combined speedups of up to 7.9 times when integrated with other parallel decoding techniques such as kv-caching and multi-token unmasking."
  },
  {
    "id": 571,
    "title": "Inverting Trojans in LLMs",
    "body": "While effective backdoor detection and inversion schemes have been developed\nfor AIs used e.g. for images, there are challenges in \"porting\" these methods\nto LLMs. First, the LLM input space is discrete, which precludes gradient-based\nsearch over this space, central to many backdoor inversion methods. Second,\nthere are ~30,000^k k-tuples to consider, k the token-length of a putative\ntrigger. Third, for LLMs there is the need to blacklist tokens that have strong\nmarginal associations with the putative target response (class) of an attack,\nas such tokens give false detection signals. However, good blacklists may not\nexist for some domains. We propose a LLM trigger inversion approach with three\nkey components: i) discrete search, with putative triggers greedily accreted,\nstarting from a select list of singletons; ii) implicit blacklisting, achieved\nby evaluating the average cosine similarity, in activation space, between a\ncandidate trigger and a small clean set of samples from the putative target\nclass; iii) detection when a candidate trigger elicits high misclassifications,\nand with unusually high decision confidence. Unlike many recent works, we\ndemonstrate that our approach reliably detects and successfully inverts\nground-truth backdoor trigger phrases.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.16203v1",
    "published_at": "2025-09-19T17:59:57",
    "created_at": "2025-09-22T09:00:33.977420",
    "key_finding": "this research introduces a novel methodology for detecting and inverting backdoor triggers in large language models (llms) that effectively addresses the unique challenges posed by discrete token spaces.",
    "methodology": "the approach leverages a greedy discrete search for triggers, employs implicit blacklisting using cosine similarity in activation space, and identifies triggers based on their potential to misclassify with high confidence.",
    "implications": "the proposed method enhances the reliability of backdoor trigger detection and inversion in llms, paving the way for improved security measures in language-based ai systems."
  },
  {
    "id": 572,
    "title": "RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation",
    "body": "Large language models excel at function- and file-level code generation, yet\ngenerating complete repositories from scratch remains a fundamental challenge.\nThis process demands coherent and reliable planning across proposal- and\nimplementation-level stages, while natural language, due to its ambiguity and\nverbosity, is ill-suited for faithfully representing complex software\nstructures. To address this, we introduce the Repository Planning Graph (RPG),\na persistent representation that unifies proposal- and implementation-level\nplanning by encoding capabilities, file structures, data flows, and functions\nin one graph. RPG replaces ambiguous natural language with an explicit\nblueprint, enabling long-horizon planning and scalable repository generation.\nBuilding on RPG, we develop ZeroRepo, a graph-driven framework for repository\ngeneration from scratch. It operates in three stages: proposal-level planning\nand implementation-level refinement to construct the graph, followed by\ngraph-guided code generation with test validation. To evaluate this setting, we\nconstruct RepoCraft, a benchmark of six real-world projects with 1,052 tasks.\nOn RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly\n3.9$\\times$ the strongest baseline (Claude Code) and about 64$\\times$ other\nbaselines. It attains 81.5% functional coverage and a 69.7% pass rate,\nexceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further\nanalysis shows that RPG models complex dependencies, enables progressively more\nsophisticated planning through near-linear scaling, and enhances LLM\nunderstanding of repositories, thereby accelerating agent localization.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.16198v1",
    "published_at": "2025-09-19T17:58:14",
    "created_at": "2025-09-22T09:00:37.118075",
    "key_finding": "**** the repository planning graph (rpg) enables enhanced long-horizon planning and scalable code repository generation, outperforming existing models significantly in both code and functional coverage.",
    "methodology": "",
    "implications": "**methodology:** rpg integrates proposal- and implementation-level planning into a single graph representation that facilitates a three-stage repository generation process in the zerorepo framework, validated using the repocraft benchmark comprising real-world projects."
  },
  {
    "id": 573,
    "title": "a scalable multimodal model with a hybrid vision tokenizer",
    "body": "Unified multimodal Large Language Models (LLMs) that can both understand and\ngenerate visual content hold immense potential. However, existing open-source\nmodels often suffer from a performance trade-off between these capabilities. We\npresent Manzano, a simple and scalable unified framework that substantially\nreduces this tension by coupling a hybrid image tokenizer with a well-curated\ntraining recipe. A single shared vision encoder feeds two lightweight adapters\nthat produce continuous embeddings for image-to-text understanding and discrete\ntokens for text-to-image generation within a common semantic space. A unified\nautoregressive LLM predicts high-level semantics in the form of text and image\ntokens, with an auxiliary diffusion decoder subsequently translating the image\ntokens into pixels. The architecture, together with a unified training recipe\nover understanding and generation data, enables scalable joint learning of both\ncapabilities. Manzano achieves state-of-the-art results among unified models,\nand is competitive with specialist models, particularly on text-rich\nevaluation. Our studies show minimal task conflicts and consistent gains from\nscaling model size, validating our design choice of a hybrid tokenizer.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.16197v1",
    "published_at": "2025-09-19T17:58:00",
    "created_at": "2025-09-22T09:00:42.700451",
    "key_finding": "manzano is a unified multimodal model that effectively integrates visual content understanding and generation, outperforming existing models in performance while minimizing trade-offs.",
    "methodology": "",
    "implications": "methodology: the framework utilizes a hybrid image tokenizer coupled with a shared vision encoder and lightweight adapters to produce continuous embeddings for comprehension and discrete tokens for generation, all trained collaboratively through a unified recipe."
  },
  {
    "id": 574,
    "title": "FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal Distillation",
    "body": "Neural audio codecs are a fundamental component of modern generative audio\npipelines. Although recent codecs achieve strong low-bitrate reconstruction and\nprovide powerful representations for downstream tasks, most are non-streamable,\nlimiting their use in real-time applications. We present FocalCodec-Stream, a\nhybrid codec based on focal modulation that compresses speech into a single\nbinary codebook at 0.55 - 0.80 kbps with a theoretical latency of 80 ms. Our\napproach combines multi-stage causal distillation of WavLM with targeted\narchitectural improvements, including a lightweight refiner module that\nenhances quality under latency constraints. Experiments show that\nFocalCodec-Stream outperforms existing streamable codecs at comparable\nbitrates, while preserving both semantic and acoustic information. The result\nis a favorable trade-off between reconstruction quality, downstream task\nperformance, latency, and efficiency. Code and checkpoints will be released at\nhttps://github.com/lucadellalib/focalcodec.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.16195v1",
    "published_at": "2025-09-19T17:57:13",
    "created_at": "2025-09-22T09:00:45.410731",
    "key_finding": "**** focalcodec-stream is a hybrid codec that achieves low-bitrate speech compression (0.55 - 0.80 kbps) with reduced latency (80 ms), outperforming existing streamable codecs in both quality and efficiency.",
    "methodology": "",
    "implications": "**methodology:** the codec employs multi-stage causal distillation of wavlm and incorporates architectural enhancements such as a lightweight refiner module to improve output quality while maintaining low latency."
  },
  {
    "id": 575,
    "title": "latent learning: how episodic memory aids parametric learning",
    "body": "When do machine learning systems fail to generalize, and what mechanisms\ncould improve their generalization? Here, we draw inspiration from cognitive\nscience to argue that one weakness of machine learning systems is their failure\nto exhibit latent learning -- learning information that is not relevant to the\ntask at hand, but that might be useful in a future task. We show how this\nperspective links failures ranging from the reversal curse in language modeling\nto new findings on agent-based navigation. We then highlight how cognitive\nscience points to episodic memory as a potential part of the solution to these\nissues. Correspondingly, we show that a system with an oracle retrieval\nmechanism can use learning experiences more flexibly to generalize better\nacross many of these challenges. We also identify some of the essential\ncomponents for effectively using retrieval, including the importance of\nwithin-example in-context learning for acquiring the ability to use information\nacross retrieved examples. In summary, our results illustrate one possible\ncontributor to the relative data inefficiency of current machine learning\nsystems compared to natural intelligence, and help to understand how retrieval\nmethods can complement parametric learning to improve generalization.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.16189v1",
    "published_at": "2025-09-19T17:49:25",
    "created_at": "2025-09-22T09:00:51.702805",
    "key_finding": "the research demonstrates that incorporating episodic memory into machine learning systems enhances their ability to generalize by enabling flexible reuse of seemingly irrelevant experiences.",
    "methodology": "the authors draw parallels between cognitive science principles and challenges in machine learning, using an oracle retrieval mechanism to showcase improved generalization across various tasks.",
    "implications": "this study suggests that integrating latent learning and episodic memory can address inefficiencies in current machine learning approaches, ultimately making them more akin to natural intelligence."
  },
  {
    "id": 566,
    "title": "Out-of-Sight Trajectories: Tracking, Fusion, and Prediction",
    "body": "Trajectory prediction is a critical task in computer vision and autonomous\nsystems, playing a key role in autonomous driving, robotics, surveillance, and\nvirtual reality. Existing methods often rely on complete and noise-free\nobservational data, overlooking the challenges associated with out-of-sight\nobjects and the inherent noise in sensor data caused by limited camera\ncoverage, obstructions, and the absence of ground truth for denoised\ntrajectories. These limitations pose safety risks and hinder reliable\nprediction in real-world scenarios. In this extended work, we present\nadvancements in Out-of-Sight Trajectory (OST), a novel task that predicts the\nnoise-free visual trajectories of out-of-sight objects using noisy sensor data.\nBuilding on our previous research, we broaden the scope of Out-of-Sight\nTrajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending\nits applicability to autonomous driving, robotics, surveillance, and virtual\nreality. Our enhanced Vision-Positioning Denoising Module leverages camera\ncalibration to establish a vision-positioning mapping, addressing the lack of\nvisual references, while effectively denoising noisy sensor data in an\nunsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB\ndatasets, our approach achieves state-of-the-art performance in both trajectory\ndenoising and prediction, significantly surpassing previous baselines.\nAdditionally, we introduce comparisons with traditional denoising methods, such\nas Kalman filtering, and adapt recent trajectory prediction models to our task,\nproviding a comprehensive benchmark. This work represents the first initiative\nto integrate vision-positioning projection for denoising noisy sensor\ntrajectories of out-of-sight agents, paving the way for future advances. The\ncode and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.15219v1",
    "published_at": "2025-09-18T17:59:16",
    "created_at": "2025-09-19T09:00:33.029689",
    "key_finding": "**** this research introduces a novel out-of-sight trajectory (ost) prediction framework that accurately estimates noise-free trajectories of out-of-sight objects using noisy sensor data, achieving state-of-the-art results.",
    "methodology": "",
    "implications": "**methodology:** the study employs an enhanced vision-positioning denoising module that combines camera calibration with an unsupervised denoising approach, and evaluates its effectiveness on vi-fi and jrdb datasets while comparing it to traditional methods like kalman filtering."
  },
  {
    "id": 567,
    "title": "Generalizable Geometric Image Caption Synthesis",
    "body": "Multimodal large language models have various practical applications that\ndemand strong reasoning abilities. Despite recent advancements, these models\nstill struggle to solve complex geometric problems. A key challenge stems from\nthe lack of high-quality image-text pair datasets for understanding geometric\nimages. Furthermore, most template-based data synthesis pipelines typically\nfail to generalize to questions beyond their predefined templates. In this\npaper, we bridge this gap by introducing a complementary process of\nReinforcement Learning with Verifiable Rewards (RLVR) into the data generation\npipeline. By adopting RLVR to refine captions for geometric images synthesized\nfrom 50 basic geometric relations and using reward signals derived from\nmathematical problem-solving tasks, our pipeline successfully captures the key\nfeatures of geometry problem-solving. This enables better task generalization\nand yields non-trivial improvements. Furthermore, even in out-of-distribution\nscenarios, the generated dataset enhances the general reasoning capabilities of\nmultimodal large language models, yielding accuracy improvements of\n$2.8\\%\\text{-}4.8\\%$ in statistics, arithmetic, algebraic, and numerical tasks\nwith non-geometric input images of MathVista and MathVerse, along with\n$2.4\\%\\text{-}3.9\\%$ improvements in Art, Design, Tech, and Engineering tasks\nin MMMU.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.15217v1",
    "published_at": "2025-09-18T17:59:11",
    "created_at": "2025-09-19T09:00:35.953442",
    "key_finding": "the introduction of a reinforcement learning with verifiable rewards (rlvr) process in generating image-text pairs significantly improves reasoning capabilities for geometric problems in multimodal large language models.",
    "methodology": "the study synthesizes caption data from basic geometric relations using rlvr to optimize captions based on reward signals from mathematical problem-solving tasks, enhancing generalization across various scenarios.",
    "implications": "this approach not only addresses the inadequacy of existing datasets but also leads to measurable accuracy improvements in a range of mathematical and design-related tasks, demonstrating the potential for enhanced reasoning in ai applications."
  },
  {
    "id": 568,
    "title": "Explicit Context-Driven Neural Acoustic Modeling for High-Fidelity RIR Generation",
    "body": "Realistic sound simulation plays a critical role in many applications. A key\nelement in sound simulation is the room impulse response (RIR), which\ncharacterizes how sound propagates from a source to a listener within a given\nspace. Recent studies have applied neural implicit methods to learn RIR using\ncontext information collected from the environment, such as scene images.\nHowever, these approaches do not effectively leverage explicit geometric\ninformation from the environment. To further exploit the potential of neural\nimplicit models with direct geometric features, we present Mesh-infused Neural\nAcoustic Field (MiNAF), which queries a rough room mesh at given locations and\nextracts distance distributions as an explicit representation of local context.\nOur approach demonstrates that incorporating explicit local geometric features\ncan better guide the neural network in generating more accurate RIR\npredictions. Through comparisons with conventional and state-of-the-art\nbaseline methods, we show that MiNAF performs competitively across various\nevaluation metrics. Furthermore, we verify the robustness of MiNAF in datasets\nwith limited training samples, demonstrating an advance in high-fidelity sound\nsimulation.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.15210v1",
    "published_at": "2025-09-18T17:57:07",
    "created_at": "2025-09-19T09:00:39.123053",
    "key_finding": "the mesh-infused neural acoustic field (minaf) improves room impulse response (rir) generation by effectively utilizing explicit geometric information from a rough room mesh.",
    "methodology": "minaf queries local geometric features to extract distance distributions, guiding a neural network for more accurate rir predictions compared to conventional and state-of-the-art methods.",
    "implications": "this research demonstrates the potential for enhanced high-fidelity sound simulation, even in scenarios with limited training data, thereby advancing applications reliant on realistic audio environments."
  },
  {
    "id": 569,
    "title": "FlowRL: Matching Reward Distributions for LLM Reasoning",
    "body": "We propose FlowRL: matching the full reward distribution via flow balancing\ninstead of maximizing rewards in large language model (LLM) reinforcement\nlearning (RL). Recent advanced reasoning models adopt reward-maximizing methods\n(\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while\nneglecting less frequent but valid reasoning paths, thus reducing diversity. In\ncontrast, we transform scalar rewards into a normalized target distribution\nusing a learnable partition function, and then minimize the reverse KL\ndivergence between the policy and the target distribution. We implement this\nidea as a flow-balanced optimization method that promotes diverse exploration\nand generalizable reasoning trajectories. We conduct experiments on math and\ncode reasoning tasks: FlowRL achieves a significant average improvement of\n$10.0\\%$ over GRPO and $5.1\\%$ over PPO on math benchmarks, and performs\nconsistently better on code reasoning tasks. These results highlight reward\ndistribution-matching as a key step toward efficient exploration and diverse\nreasoning in LLM reinforcement learning.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.15207v1",
    "published_at": "2025-09-18T17:56:36",
    "created_at": "2025-09-19T09:00:41.839524",
    "key_finding": "flowrl significantly enhances reasoning diversity in large language model reinforcement learning by matching reward distributions rather than merely maximizing scalar rewards.",
    "methodology": "the approach employs a flow-balanced optimization technique that transforms scalar rewards into a normalized target distribution, minimizing the reverse kl divergence between the policy and this target.",
    "implications": "this method leads to improved performance in both math and code reasoning tasks, suggesting that reward distribution-matching is essential for promoting efficient exploration and generalizable reasoning in llms."
  },
  {
    "id": 570,
    "title": "CausalPre: Scalable and Effective Data Pre-processing for Causal Fairness",
    "body": "Causal fairness in databases is crucial to preventing biased and inaccurate\noutcomes in downstream tasks. While most prior work assumes a known causal\nmodel, recent efforts relax this assumption by enforcing additional\nconstraints. However, these approaches often fail to capture broader attribute\nrelationships that are critical to maintaining utility. This raises a\nfundamental question: Can we harness the benefits of causal reasoning to design\nefficient and effective fairness solutions without relying on strong\nassumptions about the underlying causal model? In this paper, we seek to answer\nthis question by introducing CausalPre, a scalable and effective\ncausality-guided data pre-processing framework that guarantees justifiable\nfairness, a strong causal notion of fairness. CausalPre extracts causally fair\nrelationships by reformulating the originally complex and computationally\ninfeasible extraction task into a tailored distribution estimation problem. To\nensure scalability, CausalPre adopts a carefully crafted variant of\nlow-dimensional marginal factorization to approximate the joint distribution,\ncomplemented by a heuristic algorithm that efficiently tackles the associated\ncomputational challenge. Extensive experiments on benchmark datasets\ndemonstrate that CausalPre is both effective and scalable, challenging the\nconventional belief that achieving causal fairness requires trading off\nrelationship coverage for relaxed model assumptions.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.15199v1",
    "published_at": "2025-09-18T17:54:01",
    "created_at": "2025-09-19T09:00:44.802778",
    "key_finding": "causalpre introduces a novel framework for achieving justifiable causal fairness in data preprocessing without relying on strong assumptions about the underlying causal models.",
    "methodology": "the framework reformulates the extraction of causally fair relationships into a distribution estimation problem and employs low-dimensional marginal factorization alongside a heuristic algorithm to ensure computational efficiency and scalability.",
    "implications": "the results demonstrate that causalpre effectively balances relationship coverage and the relaxation of model assumptions, challenging the belief that achieving causal fairness necessitates compromising on data utility."
  },
  {
    "id": 561,
    "title": "Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision",
    "body": "Where do learning signals come from when there is no ground truth in\npost-training? We propose turning exploration into supervision through Compute\nas Teacher (CaT), which converts the model's own exploration at inference-time\ninto reference-free supervision by synthesizing a single reference from a group\nof parallel rollouts and then optimizing toward it. Concretely, the current\npolicy produces a group of rollouts; a frozen anchor (the initial policy)\nreconciles omissions and contradictions to estimate a reference, turning extra\ninference-time compute into a teacher signal. We turn this into rewards in two\nregimes: (i) verifiable tasks use programmatic equivalence on final answers;\n(ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria\nscored by an independent LLM judge, with reward given by the fraction\nsatisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge\nscores), synthesis may disagree with the majority and be correct even when all\nrollouts are wrong; performance scales with the number of rollouts. As a\ntest-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up\nto +27% on MATH-500; +12% on HealthBench). With reinforcement learning\n(CaT-RL), we obtain further gains (up to +33% and +30%), with the trained\npolicy surpassing the initial teacher signal.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.14234v1",
    "published_at": "2025-09-17T17:59:42",
    "created_at": "2025-09-18T09:00:41.524085",
    "key_finding": "the compute as teacher (cat) approach transforms inference-time exploration into effective reference-free supervision, leading to significant performance improvements in models like gemma and qwen.",
    "methodology": "cat synthesizes a single reference from multiple rollouts of a frozen policy, using it to derive rewards for both verifiable and non-verifiable tasks, optimizing the learning process without ground truth.",
    "implications": "by leveraging additional inference-time compute, cat enhances model performance markedly, offering a novel way to enable learning in scenarios lacking explicit supervision and showing promise for future applications in reinforcement learning."
  },
  {
    "id": 562,
    "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language Environments",
    "body": "We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.14233v1",
    "published_at": "2025-09-17T17:59:21",
    "created_at": "2025-09-18T09:00:44.636468",
    "key_finding": "apertus provides a suite of fully open large language models that prioritize data compliance and enhance multilingual representation, achieving competitive performance on multilingual benchmarks.",
    "methodology": "the models are pretrained on openly available data, respecting content-owner rights and employing the goldfish objective to reduce memorization while covering over 1800 languages with 15t tokens, where ~40% of the data is non-english.",
    "implications": "by releasing all development artifacts under a permissive license, apertus facilitates transparent auditing and extensions, promoting a more ethical and inclusive approach to llm development in diverse language environments."
  },
  {
    "id": 563,
    "title": "NIRVANA: Structured pruning reimagined for large language models compression",
    "body": "Structured pruning of large language models (LLMs) offers substantial\nefficiency improvements by removing entire hidden units, yet current approaches\noften suffer from significant performance degradation, particularly in\nzero-shot settings, and necessitate costly recovery techniques such as\nsupervised fine-tuning (SFT) or adapter insertion. To address these critical\nshortcomings, we introduce NIRVANA, a novel pruning method explicitly designed\nto balance immediate zero-shot accuracy preservation with robust fine-tuning\ncapability. Leveraging a first-order saliency criterion derived from the Neural\nTangent Kernel under Adam optimization dynamics, NIRVANA provides a\ntheoretically grounded pruning strategy that respects essential model training\nbehaviors. To further address the unique challenges posed by structured\npruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across\nlayers and modules (attention vs. MLP), which adjusts pruning intensity between\nmodules in a globally balanced manner. Additionally, to mitigate the high\nsensitivity of pruning decisions to calibration data quality, we propose a\nsimple yet effective KL divergence-based calibration data selection strategy,\nensuring more reliable and task-agnostic pruning outcomes. Comprehensive\nexperiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA\noutperforms existing structured pruning methods under equivalent sparsity\nconstraints, providing a theoretically sound and practical approach to LLM\ncompression. The code is available at\nhttps://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.14230v1",
    "published_at": "2025-09-17T17:59:00",
    "created_at": "2025-09-18T09:00:47.906710",
    "key_finding": "nirvana significantly improves structured pruning of large language models, achieving higher zero-shot accuracy preservation and robust fine-tuning compared to existing methods.",
    "methodology": "",
    "implications": "methodology: the approach utilizes a first-order saliency criterion from neural tangent kernel dynamics and incorporates an adaptive sparsity allocation mechanism to optimize pruning across different model layers."
  },
  {
    "id": 564,
    "title": "Spacing Test for Fused Lasso",
    "body": "This study addresses the unresolved problem of selecting the regularization\nparameter in the fused lasso. In particular, we extend the framework of the\nSpacing Test proposed by Tibshirani et al. to the fused lasso, providing a\ntheoretical foundation for post-selection inference by characterizing the\nselection event as a polyhedral constraint. Based on the analysis of the\nsolution path of the fused lasso using a LARS-type algorithm, we derive exact\nconditional $p$-values for the selected change-points. Our method broadens the\napplicability of the Spacing Test from the standard lasso to fused penalty\nstructures. Furthermore, through numerical experiments comparing the proposed\nmethod with sequential versions of AIC and BIC as well as cross-validation, we\ndemonstrate that the proposed approach properly controls the type I error while\nachieving high detection power. This work offers a theoretically sound and\ncomputationally practical solution for parameter selection and post-selection\ninference in structured signal estimation problems. Keywords: Fused Lasso,\nRegularization parameter selection, Spacing Test for Lasso, Selective\ninference, Change-point detection",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.14229v1",
    "published_at": "2025-09-17T17:58:28",
    "created_at": "2025-09-18T09:00:49.754548",
    "key_finding": "the study develops a spacing test extension for the fused lasso, enabling accurate post-selection inference and regularization parameter selection.",
    "methodology": "it employs a lars-type algorithm to analyze the solution path of the fused lasso, yielding exact conditional $p$-values for the identified change-points.",
    "implications": "this approach not only enhances the reliability of change-point detection under a fused penalty but also maintains type i error control and high detection power compared to traditional methods like aic, bic, and cross-validation."
  },
  {
    "id": 565,
    "title": "Multi-robot Multi-source Localization in Complex Flows with Physics-Preserving Environment Models",
    "body": "Source localization in a complex flow poses a significant challenge for\nmulti-robot teams tasked with localizing the source of chemical leaks or\ntracking the dispersion of an oil spill. The flow dynamics can be time-varying\nand chaotic, resulting in sporadic and intermittent sensor readings, and\ncomplex environmental geometries further complicate a team's ability to model\nand predict the dispersion. To accurately account for the physical processes\nthat drive the dispersion dynamics, robots must have access to computationally\nintensive numerical models, which can be difficult when onboard computation is\nlimited. We present a distributed mobile sensing framework for source\nlocalization in which each robot carries a machine-learned, finite element\nmodel of its environment to guide information-based sampling. The models are\nused to evaluate an approximate mutual information criterion to drive an\ninfotaxis control strategy, which selects sensing regions that are expected to\nmaximize informativeness for the source localization objective. Our approach\nachieves faster error reduction compared to baseline sensing strategies and\nresults in more accurate source localization compared to baseline machine\nlearning approaches.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.14228v1",
    "published_at": "2025-09-17T17:58:25",
    "created_at": "2025-09-18T09:00:51.876927",
    "key_finding": "the proposed framework enables multi-robot teams to accurately localize sources of chemical leaks in complex flows more effectively than traditional approaches.",
    "methodology": "each robot utilizes a machine-learned finite element model of its environment to implement an infotaxis control strategy based on an approximate mutual information criterion for optimized sampling.",
    "implications": "this distributed approach enhances the potential for real-time environmental monitoring and disaster response by improving accuracy and efficiency in source localization tasks."
  },
  {
    "id": 556,
    "title": "Do Natural Language Descriptions of Model Activations Convey Privileged Information?",
    "body": "Recent interpretability methods have proposed to translate LLM internal\nrepresentations into natural language descriptions using a second verbalizer\nLLM. This is intended to illuminate how the target model represents and\noperates on inputs. But do such activation verbalization approaches actually\nprovide privileged knowledge about the internal workings of the target model,\nor do they merely convey information about its inputs? We critically evaluate\npopular verbalization methods across datasets used in prior work and find that\nthey succeed at benchmarks without any access to target model internals,\nsuggesting that these datasets are not ideal for evaluating verbalization\nmethods. We then run controlled experiments which reveal that verbalizations\noften reflect the parametric knowledge of the verbalizer LLM which generated\nthem, rather than the activations of the target LLM being decoded. Taken\ntogether, our results indicate a need for targeted benchmarks and experimental\ncontrols to rigorously assess whether verbalization methods provide meaningful\ninsights into the operations of LLMs.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.13316v1",
    "published_at": "2025-09-16T17:59:04",
    "created_at": "2025-09-17T09:00:37.508780",
    "key_finding": "recent interpretability methods using natural language descriptions of model activations do not provide privileged insights into the internal workings of target language models, as they often reflect the knowledge of the verbalizer llm instead.",
    "methodology": "the authors evaluated popular activation verbalization methods across previous benchmark datasets and conducted controlled experiments to assess the validity of the insights provided by these methods.",
    "implications": "the findings highlight the necessity for improved benchmarks and experimental controls to effectively measure the true interpretability and insights offered by activation verbalization techniques in large language models."
  },
  {
    "id": 557,
    "title": "websailor-v2: bridging to proprietary agents with ai",
    "body": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all open-source agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.13305v1",
    "published_at": "2025-09-16T17:57:03",
    "created_at": "2025-09-17T09:00:43.862890",
    "key_finding": "websailor-v2 establishes a competitive performance level in complex information-seeking tasks, comparable to proprietary agents like deepresearch.",
    "methodology": "the approach combines synthetic data generation for high-uncertainty tasks with an efficient reinforcement learning algorithm, duplicating sampling policy optimization (dupo), within a structured post-training framework.",
    "implications": "this research demonstrates that open-source models can achieve superhuman capabilities by adopting sophisticated reasoning patterns and training methodologies that address extreme uncertainty in information navigation."
  },
  {
    "id": 558,
    "title": "QDFlow: A Python package for physics simulations of quantum dot devices",
    "body": "Recent advances in machine learning (ML) have accelerated progress in\ncalibrating and operating quantum dot (QD) devices. However, most ML approaches\nrely on access to large, high-quality labeled datasets for training,\nbenchmarking, and validation, with labels capturing key features in the data.\nObtaining such datasets experimentally is challenging due to limited data\navailability and the labor-intensive nature of labeling. QDFlow is an\nopen-source physics simulator for multi-QD arrays that generates realistic\nsynthetic data with ground-truth labels. QDFlow combines a self-consistent\nThomas-Fermi solver, a dynamic capacitance model, and flexible noise modules to\nproduce charge stability diagrams and ray-based data closely resembling\nexperiments. With extensive tunable parameters and customizable noise models,\nQDFlow supports the creation of large, diverse datasets for ML development,\nbenchmarking, and quantum device research.",
    "source": "arxiv",
    "url": "http://arxiv.org/abs/2509.13298v1",
    "published_at": "2025-09-16T17:54:25",
    "created_at": "2025-09-17T09:00:46.907215",
    "key_finding": "**** qdflow is a python package that generates realistic synthetic data with ground-truth labels for quantum dot device simulations, addressing the data scarcity issue in machine learning applications.",
    "methodology": "**** the package uses a self-consistent thomas-fermi solver, a dynamic capacitance model, and customizable noise modules to create charge stability diagrams and ray-based data that mimic experimental results.",
    "implications": "**** by facilitating the generation of large, diverse labeled datasets, qdflow enhances the development and benchmarking of machine learning techniques, significantly contributing to the research and operation of quantum dot devices."
  }
]